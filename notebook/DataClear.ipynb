{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d353f5",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# FastT5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dd01193",
   "metadata": {
    "code_folding": [
     13,
     24
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def identify_string_type(s):\n",
    "    \"\"\"\n",
    "    give the unique know of a paper\n",
    "    \"\"\"\n",
    "    pattern1 = r\"^\\d+\\.\\d+$\"\n",
    "    pattern2 = r\"^[a-zA-Z]+\\/\\w+$\"\n",
    "\n",
    "    if re.match(pattern1, s):\n",
    "        return f\"ArXiv:{s}\"\n",
    "    elif re.match(pattern2, s):\n",
    "        return f\"ArXiv:{s}\"\n",
    "    else:\n",
    "        return s\n",
    "def read_jsonl(path):\n",
    "    try:\n",
    "        with open(path,'r') as f:\n",
    "            data = [json.load(f)]\n",
    "    except json.JSONDecodeError:\n",
    "        with open(path,'r') as f:\n",
    "            data = []\n",
    "            for line in f:\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def structure_dict(_dict):\n",
    "    paper_id = _dict['paper_id']\n",
    "    abstract = _dict['abstract']\n",
    "    key_words= _dict['question']\n",
    "    pattern = r'\\d+\\.\\s*(.*?)(?=\\s*\\d+\\.\\s*|$)'\n",
    "    words   = re.findall(pattern, key_words)\n",
    "    if len(words)<1:\n",
    "        words = key_words.split(',')\n",
    "    return [paper_id] + [abstract] + words[:10]\n",
    "import os\n",
    "ROOTPATH='data/unarXive/'\n",
    "all_json_list = []\n",
    "for level_1_name in os.listdir(ROOTPATH):\n",
    "    level_1_path = os.path.join(ROOTPATH, level_1_name)\n",
    "    for level_2_name in os.listdir(level_1_path):\n",
    "        level_2_path = os.path.join(level_1_path, level_2_name)\n",
    "        all_json_list.append(level_2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "effee267",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "all_data = [] \n",
    "for path in tqdm(all_json_list):\n",
    "    all_data+=read_jsonl(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c52b8f83",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91ed80d1",
   "metadata": {
    "code_folding": [
     6
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "from fastchat.utils import is_partial_stop, is_sentence_complete, get_context_length\n",
    "\n",
    "from fastchat.model.model_adapter import (\n",
    "    load_model,get_model_adapter,\n",
    "    get_conversation_template,\n",
    ")\n",
    "from fastchat.serve.inference import *\n",
    "from typing import Iterable, Optional, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e01d12a",
   "metadata": {
    "code_folding": [
     0,
     7
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_result(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    params: Dict,\n",
    "    device:str,\n",
    "    context_len: int,\n",
    "    stream_interval: int = 2,\n",
    "):\n",
    "    # Read parameters\n",
    "    prompt = params[\"prompt\"]\n",
    "    len_prompt = len(prompt)\n",
    "    temperature = float(params.get(\"temperature\", 1.0))\n",
    "    repetition_penalty = float(params.get(\"repetition_penalty\", 1.0))\n",
    "    top_p = float(params.get(\"top_p\", 1.0))\n",
    "    top_k = int(params.get(\"top_k\", -1))  # -1 means disable\n",
    "    max_new_tokens = int(params.get(\"max_new_tokens\", 256))\n",
    "    echo = bool(params.get(\"echo\", True))\n",
    "    stop_str = params.get(\"stop\", None)\n",
    "    stop_token_ids = params.get(\"stop_token_ids\", None) or []\n",
    "    stop_token_ids.append(tokenizer.eos_token_id)\n",
    "\n",
    "    logits_processor = prepare_logits_processor(\n",
    "        temperature, repetition_penalty, top_p, top_k\n",
    "    )\n",
    "\n",
    "    input_ids = tokenizer(prompt).input_ids\n",
    "    output_ids = list(input_ids)\n",
    "\n",
    "    if model.config.is_encoder_decoder:\n",
    "        max_src_len = context_len\n",
    "    else:  # truncate\n",
    "        max_src_len = context_len - max_new_tokens - 8\n",
    "\n",
    "    input_ids = input_ids[-max_src_len:]\n",
    "    input_echo_len = len(input_ids)\n",
    "\n",
    "    if model.config.is_encoder_decoder:\n",
    "        encoder_output = model.encoder(\n",
    "            input_ids=torch.as_tensor([input_ids], device=device)\n",
    "        )[0]\n",
    "        start_ids = torch.as_tensor(\n",
    "            [[model.generation_config.decoder_start_token_id]],\n",
    "            dtype=torch.int64,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    past_key_values = out = None\n",
    "    sent_interrupt = False\n",
    "    for i in range(max_new_tokens):\n",
    "        if i == 0:  # prefill\n",
    "            if model.config.is_encoder_decoder:\n",
    "                out = model.decoder(\n",
    "                    input_ids=start_ids,\n",
    "                    encoder_hidden_states=encoder_output,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "                logits = model.lm_head(out[0])\n",
    "            else:\n",
    "                out = model(torch.as_tensor([input_ids], device=device), use_cache=True)\n",
    "                logits = out.logits\n",
    "            past_key_values = out.past_key_values\n",
    "        else:  # decoding\n",
    "            if model.config.is_encoder_decoder:\n",
    "                out = model.decoder(\n",
    "                    input_ids=torch.as_tensor(\n",
    "                        [[token] if not sent_interrupt else output_ids], device=device\n",
    "                    ),\n",
    "                    encoder_hidden_states=encoder_output,\n",
    "                    use_cache=True,\n",
    "                    past_key_values=past_key_values if not sent_interrupt else None,\n",
    "                )\n",
    "                sent_interrupt = False\n",
    "\n",
    "                logits = model.lm_head(out[0])\n",
    "            else:\n",
    "                out = model(\n",
    "                    input_ids=torch.as_tensor(\n",
    "                        [[token] if not sent_interrupt else output_ids], device=device\n",
    "                    ),\n",
    "                    use_cache=True,\n",
    "                    past_key_values=past_key_values if not sent_interrupt else None,\n",
    "                )\n",
    "                sent_interrupt = False\n",
    "                logits = out.logits\n",
    "            past_key_values = out.past_key_values\n",
    "\n",
    "        if logits_processor:\n",
    "            if repetition_penalty > 1.0:\n",
    "                tmp_output_ids = torch.as_tensor([output_ids], device=logits.device)\n",
    "            else:\n",
    "                tmp_output_ids = None\n",
    "            last_token_logits = logits_processor(tmp_output_ids, logits[:, -1, :])[0]\n",
    "        else:\n",
    "            last_token_logits = logits[0, -1, :]\n",
    "\n",
    "\n",
    "        if temperature < 1e-5 or top_p < 1e-8:  # greedy\n",
    "            _, indices = torch.topk(last_token_logits, 2)\n",
    "            tokens = [int(index) for index in indices.tolist()]\n",
    "        else:\n",
    "            probs = torch.softmax(last_token_logits, dim=-1)\n",
    "            indices = torch.multinomial(probs, num_samples=2)\n",
    "            tokens = [int(token) for token in indices.tolist()]\n",
    "        token = tokens[0]\n",
    "        output_ids.append(token)\n",
    "\n",
    "        if token in stop_token_ids:\n",
    "            stopped = True\n",
    "        else:\n",
    "            stopped = False\n",
    "        if stopped: break\n",
    "\n",
    "    tmp_output_ids = output_ids[input_echo_len:]\n",
    "    rfind_start = 0\n",
    "\n",
    "    output = tokenizer.decode(\n",
    "        tmp_output_ids,\n",
    "        skip_special_tokens=True,\n",
    "        spaces_between_special_tokens=False,\n",
    "        clean_up_tokenization_spaces=True,\n",
    "    )\n",
    "    return output\n",
    "\n",
    "model_path = 'pretrain_weights/fast_t5'\n",
    "model, tokenizer = load_model(\n",
    "    model_path,\n",
    "    \"cuda\",\n",
    "    1,\n",
    "    load_8bit=False\n",
    ")\n",
    "context_len = get_context_length(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66b3e105",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d392dc73",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "whole_sentense = pd.read_csv(\"data/unarXive.clear/unarXive.clear.sections.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fbc8bce",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Flipping. We flip an image along its horizontal axis. \n"
     ]
    }
   ],
   "source": [
    "print(whole_sentense.iloc[249]['section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aaa46de",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lengths = [len(l.split()) for l in whole_sentense['section']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6080fc78",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from mltool.visualization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "901695b5",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3523960"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((length>128) * (length<2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28bfd7c4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12e8413c",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "length = np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c813205",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30076"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(length>800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c451af74",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading csv files...........\n",
      "done~!\n"
     ]
    }
   ],
   "source": [
    "print(\"loading csv files...........\")\n",
    "sentense_ids = pd.read_csv(\"data/unarXive.clear/unarXive.clear.sections.id.csv\")\n",
    "print(\"done~!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3320251",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hdf5_file = 'data/unarXive.clear/unarXive.clear.sections.h5'\n",
    "f = h5py.File(hdf5_file, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b19aea4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentenses = []\n",
    "for _id in range(1000):\n",
    "    data  = sentense_ids.iloc[_id]\n",
    "    paper_id   = data['paper_id']\n",
    "    sentence_id= data['section_num']\n",
    "    sentense = f.get(f'{paper_id}/{sentence_id}')[()].decode('utf-8')\n",
    "    sentenses.append(sentense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "867161fe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46a27c9e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7c563ee8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer.padding_side='left'\n",
    "prompts    = [\" \".join(get_prompt(i).split()) for i in range(500,500+3)]\n",
    "input_ids_f= tokenizer(prompts).input_ids\n",
    "inputs_ids = tokenizer.pad({'input_ids': input_ids_f},padding='longest',\n",
    "                            max_length=512,pad_to_multiple_of=8,return_attention_mask=False,\n",
    "                            return_tensors='pt').input_ids.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1a8c8f36",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 7185, 5, 32103, 1]]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5f057284",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_prompt(_id,max_length=128):\n",
    "    data  = sentense_ids.iloc[_id]\n",
    "    paper_id   = data['paper_id']\n",
    "    sentence_id= data['section_num']\n",
    "    sentense = f.get(f'{paper_id}/{sentence_id}')[()].decode('utf-8')\n",
    "    sentense = \" \".join(sentense.split(\" \")[:max_length])\n",
    "    conv = get_conversation_template(model_path)\n",
    "    qs = f\"\"\"Read below sentence and tell me its type. The answer should be one word and is one of type from ['Author List', 'Reference List', 'Content']. There is the sentence \\\"{sentense}\\\" \"\"\"\n",
    "    conv.append_message(conv.roles[0], qs)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "802b8dad",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05e2a0ca719498285ed7541aad45e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.213057804107666\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "B = 1\n",
    "idxes = range(5000,5100)\n",
    "outputs = []\n",
    "for _id in tqdm(idxes):\n",
    "    tokenizer.padding_side='left'\n",
    "    prompts    = [\" \".join(get_prompt(i).split()) for i in range(B*_id,B*(_id+1))]\n",
    "    input_ids_f= tokenizer(prompts).input_ids\n",
    "    inputs_ids = tokenizer.pad({'input_ids': input_ids_f},padding='longest',\n",
    "                                max_length=512,pad_to_multiple_of=8,return_attention_mask=False,\n",
    "                                return_tensors='pt').input_ids.cuda()\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(inputs_ids,max_length=1)\n",
    "    outputs.append(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
    "end = time.time()\n",
    "cost= (end - start)/(B*len(idxes))\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aacb5eb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38adcebb",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from tqdm.notebook import tqdm\n",
    "# import pandas as pd\n",
    "# import h5py\n",
    "\n",
    "# sentenses = pd.read_csv(\"data/unarXive.clear/unarXive.clear.sections.csv\")\n",
    "\n",
    "# lengths   = [len(l.split()) for l in sentenses['section']]\n",
    "# sentenses['length'] = lengths\n",
    "# sentenses = sentenses[sentenses['length']>100]\n",
    "# sentenses = sentenses[sentenses['length']<1000]\n",
    "\n",
    "# df = sentenses[['paper_id','section_num']]\n",
    "# df.to_csv(\"data/unarXive.clear/unarXive.clear.sections.good.id.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "494dff30",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4755699"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a850112f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sentenses = []\n",
    "for _id in range(1000):\n",
    "    data  = df.iloc[_id]\n",
    "    paper_id   = data['paper_id']\n",
    "    sentence_id= data['section_num']\n",
    "    sentense = f.get(f'{paper_id}/{sentence_id}')[()].decode('utf-8')\n",
    "    sentenses.append(sentense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b09b87be",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Next, we study RMSE of SSH for different antenna spacing in the presence of intense water vapor. We set SNR = 5 dB, DoA = 60o, and water vapor density to $10 \\\\frac{g}{m^3}$ . As Fig. REF demonstrates SSH with wider antenna spacing performs significantly better in the presence of intense water vapor. Nonetheless, as range increases the performance of SSH regardless of antenna spacing deterioration due to the harsh frequency selective attenuation of the channel. Additionally, even in the presence of expensively harsh frequency selective channel, SSH with 5 mm antenna spacing can achieve better than 3o accuracy at the range of 1800 m. [Figure.16 of ArXiv:2108.04932]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenses[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8805310b",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df = sentenses\n",
    "\n",
    "# hdf5_file = 'data/unarXive.clear/unarXive.clear.sections.good.h5'\n",
    "# with h5py.File(hdf5_file, 'w') as f:\n",
    "#     for _, row in tqdm(df.iterrows(),total = len(df)):\n",
    "#         paper_id    = row['paper_id']\n",
    "#         sentence_id = row['section_num']\n",
    "#         sentence    = row['section']\n",
    "\n",
    "#         # Create a group for each paper_id if it doesn't exist\n",
    "#         if str(paper_id) not in f:\n",
    "#             f.create_group(str(paper_id))\n",
    "\n",
    "#         # Store the sentence as a dataset\n",
    "#         f[f'{paper_id}'].create_dataset(str(sentence_id), data=sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28365417",
   "metadata": {},
   "source": [
    "# Vicuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02c40f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abbc5739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM,LlamaForCausalLM\n",
    "from fastchat.conversation import get_conv_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96160161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrain_weights/vicuna-7b-v1.1\n",
      "pretrain_weights/vicuna-7b-v1.1/pytorch_model.bin\n",
      "False\n",
      "pretrain_weights/vicuna-7b-v1.1/pytorch_model.bin.index.json\n",
      "True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800475ebf8bf48bfa4ff05841f5fa715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "        \"pretrain_weights/vicuna-7b-v1.1\", low_cpu_mem_usage=True, torch_dtype=torch.float16\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15408b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.bits4llama.llama import load_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b24feb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = f\"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9731360c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ...\n",
      "Found 3 unique KN Linear values.\n",
      "Warming up autotune cache ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:29<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 unique fused mlp KN values.\n",
      "Warming up autotune cache ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:17<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model = load_quant(\"pretrain_weights/vicuna-7b-v1.1\", \"pretrain_weights/vicuna-7b-v1.1/vicuna7b-4bit-128g.pt\", 4, 128,device=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6b34592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b43ba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"pretrain_weights/vicuna-7b-v1.1\", use_fast=False)\n",
    "conv = get_conv_template('vicuna_v1.1').copy()\n",
    "conv.append_message(conv.roles[0], 'A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user\\'s questions. USER: What happens to you if you eat watermelon seeds? ASSISTANT:')\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "input_ids = tokenizer([prompt,prompt]).input_ids\n",
    "input_ids = torch.as_tensor(input_ids).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7a2f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids = tokenizer([\"Who is the main character of anarcho-capitalists\"+tokenizer.eos_token,\n",
    "                       \"Where is the important place of anarcho-capitalists\"+tokenizer.eos_token]).input_ids\n",
    "label_ids = torch.as_tensor(label_ids).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb2e450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastchat.model.compression import load_compress_model\n",
    "# model = load_compress_model(model_path=\"pretrain_weights/vicuna-7b-v1.1\", device=f'cuda:0', torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2424d35a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What happens to you if you eat watermelon seeds? ASSISTANT: ASSISTANT: Eating watermelon seeds is generally safe, but they can cause some minor discomfort and potential health issues if consumed in large quantities.\n",
      "\n",
      "The seeds of watermelon are hard and can cause teeth and gum irritation if they are not properly spit out or swallowed. They can also get stuck in the throat and cause discomfort or difficulty swallowing.\n",
      "\n",
      "Additionally, watermelon seeds contain a small amount of cyanide, which is a toxic compound. However, the amount of cyanide in watermelon seeds is very small, and the body is able to metabolize and eliminate it quickly.\n",
      "\n",
      "It is important to note that consuming large quantities of watermelon seeds can cause digestive issues, such as constipation, due to the hardness of the seeds.\n",
      "\n",
      "In summary, eating watermelon seeds is generally safe, but it is important to be mindful of the potential discomfort and digestive issues they can cause.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output_ids = model.generate(input_ids,max_length=512)\n",
    "    print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb782b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What happens to you if you eat watermelon seeds? ASSISTANT: ASSISTANT: Eating watermelon seeds is generally safe for most people. However, there are a few potential side effects that some people may experience.\n",
      "\n",
      "One potential side effect of eating watermelon seeds is digestive discomfort, such as gas and bloating. This is because watermelon seeds contain oils that can irritate the digestive system and cause these symptoms.\n",
      "\n",
      "Another potential side effect of eating watermelon seeds is choking. This is because the seeds can get stuck in the throat or esophagus and cause difficulty swallowing or breathing.\n",
      "\n",
      "It's important to note that these side effects are generally mild and temporary. However, if you experience severe or persistent side effects, it's a good idea to speak with a healthcare provider.\n",
      "\n",
      "In general, watermelon seeds are a healthy and nutritious food, and they can be a good addition to a healthy diet. However, it's always a good idea to eat them in moderation and to be mindful of the potential side effects.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
    "#print(tokenizer.decode(output_ids[1], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
