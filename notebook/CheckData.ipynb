{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc54c3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/ai4earth/zhangtianning/projects/llm\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf9b33d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "question_answer = pd.read_excel('/mnt/data/oss_beijing/zhangtianning/LLM/datasets/eval_datasets/RMP12dataset.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fc96b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairdata = pd.read_json('data/eval/RMP12/pair_question_to_answer_7k.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9a3b52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_index, answer_index_list = pairdata.iloc[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5c7f180",
   "metadata": {},
   "outputs": [],
   "source": [
    "truely_question_answer_pair = pd.read_excel('/mnt/data/oss_beijing/zhangtianning/LLM/datasets/eval_datasets/RMP12dataset.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84add866",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_string = pd.read_csv('data/eval/RMP12/question/question_index_to_string.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f824a7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Question          Who used polarization entangled photon pairs f...\n",
       "question_index                                                   10\n",
       "Name: 10, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_string.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6aeec9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Question    Who used polarization entangled photon pairs f...\n",
       "Citation                           Clauser and Shimony (1978)\n",
       "DOI                               10.1088/0034-4885/41/12/002\n",
       "Name: 10, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truely_question_answer_pair.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce0ee46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_string = pd.read_csv('data/eval/RMP12/answer_7k/paper_index_of_real_paper.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "172096df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 61.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat~!\n",
      "done, cost 7.677078247070312e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_map, data = load_wave_data(['data/eval/RMP12/answer_7k/jina_answer_token.npy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0d95b4d3",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "model_name_or_path = 'pretrain_weights/models--jinaai--jina-embeddings-v2-base-en/snapshots/7302ac470bed880590f9344bfeee32ff8722d0e5'\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            model_name_or_path,\n",
    "            model_max_length=128,\n",
    "            padding_side=\"right\",use_fast=True\n",
    "        )\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.unk_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c36883",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2460312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_content = pd.read_json('data/eval/RMP12/answer_7k/paper_content_max7000token_paragraph.jsonl',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2dadba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = np.array(tokenizer(paper_content.iloc[147].content,max_length=8192,truncation=True)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c8b778fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(tokens[tokens>0] - data[147][data[147]>0]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c087af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "423"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d1437d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>sentence_row</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>147</td>\n",
       "      <td>10.1088/0034-4885/41/12/002</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bell{\\textquotesingle}s theorem. Experimental ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>148</td>\n",
       "      <td>10.1088/0034-4885/41/12/002</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bell{\\textquotesingle}s theorem. Experimental ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>149</td>\n",
       "      <td>10.1088/0034-4885/41/12/002</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bell{\\textquotesingle}s theorem. Experimental ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>150</td>\n",
       "      <td>10.1088/0034-4885/41/12/002</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bell{\\textquotesingle}s theorem. Experimental ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>151</td>\n",
       "      <td>10.1088/0034-4885/41/12/002</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bell{\\textquotesingle}s theorem. Experimental ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>152</td>\n",
       "      <td>10.1088/0034-4885/41/12/002</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bell{\\textquotesingle}s theorem. Experimental ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                     paper_id  sentence_row arxiv_id  \\\n",
       "147    147  10.1088/0034-4885/41/12/002             0      NaN   \n",
       "148    148  10.1088/0034-4885/41/12/002             1      NaN   \n",
       "149    149  10.1088/0034-4885/41/12/002             2      NaN   \n",
       "150    150  10.1088/0034-4885/41/12/002             3      NaN   \n",
       "151    151  10.1088/0034-4885/41/12/002             4      NaN   \n",
       "152    152  10.1088/0034-4885/41/12/002             5      NaN   \n",
       "\n",
       "                                                 title  \n",
       "147  Bell{\\textquotesingle}s theorem. Experimental ...  \n",
       "148  Bell{\\textquotesingle}s theorem. Experimental ...  \n",
       "149  Bell{\\textquotesingle}s theorem. Experimental ...  \n",
       "150  Bell{\\textquotesingle}s theorem. Experimental ...  \n",
       "151  Bell{\\textquotesingle}s theorem. Experimental ...  \n",
       "152  Bell{\\textquotesingle}s theorem. Experimental ...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_string.iloc[[147, 148, 149, 150, 151, 152]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e693252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# ROOTPATH='data/unarXive_quantum_physics/quantum_physics/'\n",
    "# all_json_list = []\n",
    "# for level_1_name in os.listdir(ROOTPATH):\n",
    "#     level_1_path = os.path.join(ROOTPATH, level_1_name)\n",
    "#     for level_2_name in os.listdir(level_1_path):\n",
    "#         level_2_path = os.path.join(level_1_path, level_2_name)\n",
    "#         all_json_list.append(level_2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837ddfe7",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Merge jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "573c4ce5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import pathlib\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f840b786",
   "metadata": {
    "code_folding": [
     3,
     12,
     13
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ### old_way\n",
    "# import concurrent.futures\n",
    "# import pandas as pd\n",
    "# from tqdm.notebook import tqdm\n",
    "# def process_paper(paper):\n",
    "#     sections_list = []\n",
    "#     metadata = paper.metadata\n",
    "#     name = identify_string_type(metadata['id'])\n",
    "#     sections = paper.all_text\n",
    "#     for section_num, sec in enumerate(sections):\n",
    "#         sections_list.append([name, section_num, sec])\n",
    "#     return sections_list\n",
    "\n",
    "# def process_json_file(json_path):\n",
    "#     with open(json_path) as f:\n",
    "#         papers = pd.read_json(path_or_buf=f, lines=True)\n",
    "#     allsections = []\n",
    "#     for i in range(len(papers)):\n",
    "#         paper = papers.iloc[i]\n",
    "#         allsections.extend(process_paper(paper))\n",
    "#     return allsections\n",
    "\n",
    "# all_sections = []\n",
    "# for json_path in all_json_list:\n",
    "#     all_sections.extend(process_json_file(json_path))\n",
    "# # with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "# #     for sections in tqdm(executor.map(process_json_file, all_json_list), total=len(all_json_list)):\n",
    "# #         all_sections.extend(sections)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44b18720",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"data/unarXive_quantum_physics/quantum_physics/qt_phy_papers_1930_2123.json\",'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0f7f0f8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "THEPATH=\"data/unarXive_quantum_physics/quantum_physics/\"\n",
    "all_json_list = [os.path.join(THEPATH,t) for t in os.listdir(THEPATH) if \"qt_phy_papers\" in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e08c1421",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d725d40774c488ba40b78d66c892da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### new_way\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "allsections = []\n",
    "\n",
    "def process_paper(name,sections ):\n",
    "    sections_list = []\n",
    "    name          = identify_string_type(name)\n",
    "    for section_num, sec in enumerate(sections):\n",
    "        sections_list.append([name, section_num, sec])\n",
    "    return sections_list\n",
    "\n",
    "def process_json_file(json_path):\n",
    "    with open(json_path,'r') as f:\n",
    "        papers = json.load(f)\n",
    "    sections = []\n",
    "    for name, sentense_list in papers.items():\n",
    "        sections.extend(process_paper(name,sentense_list))\n",
    "    return sections\n",
    "\n",
    "all_sections = []\n",
    "for json_path in tqdm(all_json_list):\n",
    "    all_sections.extend(process_json_file(json_path))\n",
    "# with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "#     for sections in tqdm(executor.map(process_json_file, all_json_list), total=len(all_json_list)):\n",
    "#         all_sections.extend(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d8f19b6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b848e39",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import multiprocessing as mp\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0ff5bdd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f.close()\n",
    "hdf5_file = 'data/unarXive_full.clear.abstract.h5'\n",
    "with h5py.File(hdf5_file, 'w'):pass\n",
    "f = h5py.File(hdf5_file, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de659633",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def write_row_to_hdf5(row):\n",
    "    #with  as f:\n",
    "    paper_id,sentence_id,sentence = row\n",
    "    if str(paper_id) not in f:\n",
    "        f.create_group(str(paper_id))\n",
    "    # Store the sentence as a dataset\n",
    "    f[f'{paper_id}'].create_dataset(str(sentence_id), data=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3cc79356",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e349ed487c23469d9dfc5b5f7a84d1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2710382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for row in tqdm(all_sections):\n",
    "    write_row_to_hdf5(row)# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d7cfb2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### 脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9bbe30c1",
   "metadata": {
    "code_folding": [
     19,
     30,
     33,
     37,
     48,
     71,
     72,
     88,
     107,
     114,
     142,
     151,
     152,
     165,
     211,
     238,
     338,
     346,
     355
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import pathlib\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "import tqdm\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "import argparse\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "prepositions = {'at', 'in', 'on', 'for', 'with', 'and', 'or', 'nor', 'about', 'as', 'by', 'over', 'according to', 'against', 'along', 'among', 'apart from', 'around', 'as for', 'aside from', 'because of', 'before', 'behind', 'below', 'beneath', 'beside', 'between', 'beyond', 'but', 'by means of', 'concerning', 'despite', 'down', 'due to', 'during', 'except', 'except for', 'in addition to', 'in case of', 'in front of', 'in place of', 'in spite of', 'inside', 'instead of', 'into', 'like', 'near', 'next', 'off', 'onto', 'out', 'out of', 'outside', 'over', 'past', 'since', 'through', 'throughout', 'toward', 'under', 'underneath', 'until', 'up', 'upon', 'with', 'within', 'without'}\n",
    "prepositions = prepositions|set([t.upper() for t in prepositions])\n",
    "prepositions = prepositions|set([t[0].upper()+t[1:] for t in prepositions])\n",
    "prepositions = prepositions|set(['\\\\.', \"\\\\;\",'\\\\,'])\n",
    "\n",
    "# format function\n",
    "def format_text(example_text, insert_data, *args):\n",
    "    placeholders = re.findall(r\"\\{\\{([^{}]+)\\}\\}\", example_text)\n",
    "\n",
    "    # Replace the placeholders with corresponding values\n",
    "    for placeholder in placeholders:\n",
    "        key = \"{{\" + placeholder + \"}}\"\n",
    "        if placeholder in placeholders:\n",
    "            example_text = example_text.replace(key, insert_data[placeholder])\n",
    "    \n",
    "    return example_text\n",
    "\n",
    "def replace_continuous_spaces(text):\n",
    "    return re.sub(r'(?<=\\S)\\s+', ' ', text)\n",
    "\n",
    "def replace_start_spaces(text):\n",
    "    return text#re.sub(r'^\\s+', '[SENTENCE_START]', text)\n",
    "\n",
    "\n",
    "def merge_citations(input_str):\n",
    "    citation_prefixes = [\"DOI:\", \"ArXiv:\", \"Ref.\", \"ALEX:\"]\n",
    "    pattern = r\"\\[\\s*(?:\" + \"|\".join(citation_prefixes) + r\")\\s*\\S+\\s*(?:of\\s+\\S+)?\\s*\\]\"\n",
    "    # Find all citation marks in the input string\n",
    "    citations = re.findall(pattern, input_str)\n",
    "    #print(citations)\n",
    "    # Replace continuous citation marks with a single mark containing all citations\n",
    "    output_str = re.sub(pattern + r\"(?:\\s*,\\s*\" + pattern + r\")+\", lambda match: \", \".join(match.group(0).replace('[', '').replace(']', '').split(', ')).join(['[', ']']), input_str)\n",
    "\n",
    "    return output_str\n",
    "\n",
    "def replace_number_citation_marks(text, paper_id):\n",
    "    \n",
    "    def replace_match(match):\n",
    "        position, name, citation = match.groups()\n",
    "        #print(position)\n",
    "        # Case 1: Beginning of the sentence\n",
    "        is_start_of_string = match.start() == 0 or text[match.start() - 2] in {'.', '!', '?', ';'}\n",
    "\n",
    "        if is_start_of_string or position == '':\n",
    "            return f'[Ref.{citation} of {paper_id}]'\n",
    "        \n",
    "        # Case 2: End of the sentence\n",
    "        if position in prepositions:\n",
    "            return f'{position} [Ref.{citation} of {paper_id}]'\n",
    "        \n",
    "        # Case 3: Middle of the sentence\n",
    "        return f'(See [Ref.{citation} of {paper_id}])'\n",
    "\n",
    "    pattern = r'(?:(\\b(?:' + '|'.join(prepositions) + r')\\b) )?\\[([\\d, -]+)\\]'\n",
    "    pattern = r'(?:(\\b(?:' + '|'.join(prepositions) + r')\\b) )?(\\w+)?(?:\\s)?(?:et al\\.\\s)?\\[([\\d, -]+)\\]'\n",
    "    return re.sub(pattern, replace_match, text)\n",
    "\n",
    "\n",
    "def replace_string_citation_marks(text):\n",
    "    def replace_match(match):\n",
    "        position, citation = match.groups()\n",
    "        is_start_of_string = match.start() == 0 or text[match.start() - 2] in {'.', '!', '?', ';'}\n",
    "\n",
    "        if is_start_of_string:\n",
    "            return citation\n",
    "        # Ignore if the citation is preceded by a preposition\n",
    "        if position and position.lower() in prepositions:\n",
    "            return f'{position} {citation}'\n",
    "        return f'(See {citation})'\n",
    "\n",
    "    #pattern = r'(?:(\\b(?:' + '|'.join(prepositions) + r')\\b) )?\\[([\\d, -]+)\\]'\n",
    "    citation_prefixes = [\"DOI:\", \"ArXiv:\", \"Ref.\", \"ALEX:\"]\n",
    "    pattern = r\"(?:(\\b(?:\" + '|'.join(prepositions) + r\")\\b) )?(\\[\\s*(?:(?:\" + \"|\".join(citation_prefixes) + r\")\\s*\\S+\\s*(?:of\\s+\\S+)?\\s*(?:,\\s*)?)+\\])\".format(\"|\".join(prepositions))\n",
    "    return re.sub(pattern, replace_match, text)\n",
    "\n",
    "def format_text_with_values(text, values, type_list,paper_id):\n",
    "    \"\"\"\n",
    "    repalce all text that have the format {{ hash_code }}\n",
    "    \n",
    "    \"\"\"\n",
    "    pattern = r\"(?:Ref\\.\\s*)?\\{\\{(\" + \"|\".join(type_list) + r\"):([^{}]+)\\}\\}\"\n",
    "    def replace(match):\n",
    "        key = \"{}:{}\".format(match.group(1), match.group(2))\n",
    "        return values.get(key, key)\n",
    "\n",
    "    formatted_text = re.sub(pattern, replace, text)\n",
    "    \n",
    "    formatted_text = replace_continuous_spaces(formatted_text)\n",
    "    formatted_text = replace_start_spaces(formatted_text)\n",
    "    formatted_text = merge_citations(formatted_text)\n",
    "    formatted_text = replace_string_citation_marks(formatted_text)\n",
    "    formatted_text = replace_number_citation_marks(formatted_text,paper_id)\n",
    "    return formatted_text\n",
    "\n",
    "def split_by_indentation(all_text):\n",
    "    all_text = \"\".join(all_text)\n",
    "    all_text = all_text.replace(\"\\n\",\" \")\n",
    "    #all_text = all_text.split('[SENTENCE_START]')\n",
    "    all_text = re.split(r'\\s{2,}', all_text)\n",
    "    return all_text\n",
    "# concatenate function\n",
    "def concatenate_text(all_text, body_text, metadata, encoding=None):  \n",
    "    \n",
    "    new_text = split_by_indentation(all_text)\n",
    "    if len(new_text) == 1:\n",
    "        # this mean indentation split fail. go back normal split\n",
    "        pass\n",
    "    else:\n",
    "        all_text = new_text\n",
    "        \n",
    "    sec_name = None\n",
    "\n",
    "\n",
    "#     if len(all_text) == 0:\n",
    "#         return [], []\n",
    "#     else:\n",
    "#         # concat all the incompleted sentences\n",
    "#         all_text, sec_name = concat_by_complete_sentence(all_text, sec_name)\n",
    "    \n",
    "    if encoding is None:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = [len(encoding.encode(x, disallowed_special=())) for x in all_text]\n",
    "\n",
    "    #### get the concatenated text lists\n",
    "    ## do not limit size , we will deal with exceed token problem later\n",
    "    #cat_text, cat_sec_name = concat_by_token(all_text, sec_name, metadata, tokens)\n",
    "    cat_text, cat_sec_name = all_text, sec_name\n",
    "    return cat_text, cat_sec_name\n",
    "\n",
    "def merge_sentences(sentences):\n",
    "    merged = []\n",
    "    for sentence in sentences:\n",
    "        if merged and sentence[0].islower():\n",
    "            merged[-1] += ' ' + sentence\n",
    "        else:\n",
    "            merged.append(sentence)\n",
    "    return merged\n",
    "\n",
    "def concat_by_complete_sentence(sentences, sec_number=None):\n",
    "    \"\"\"\n",
    "     realized function:\n",
    "         merge this sentence into last sentence if the starting of this sentence is lower alphabeta\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    for sentence in sentences:\n",
    "        if merged and sentence[0].islower():\n",
    "            merged[-1] += ' ' + sentence\n",
    "        else:\n",
    "            merged.append(sentence)\n",
    "    \n",
    "    return merged, list(range(len(merged)))   \n",
    "\n",
    "def concat_by_token(all_text, sec_name, metadata, tokens, min_token = 128, max_token_soft=512):\n",
    "    # concatenate text to make it\n",
    "    #cat_text = [metadata.get('abstract', None).replace(\"\\n\",\" \") or \"\" ]\n",
    "    #cat_sec_name = ['abstract']\n",
    "    cat_text = [] \n",
    "    cat_sec_name=[]\n",
    "    ### we dont need abstract\n",
    "    \n",
    "    current_text = \"\"\n",
    "    current_sec = sec_name[0]\n",
    "    current_token = 0\n",
    "    index = 0\n",
    "    for i in range(len(all_text)):\n",
    "        if current_sec != sec_name[i]:\n",
    "            if current_token < min_token:\n",
    "                # if too short, append it to the previous paragraph\n",
    "                cat_text[-1] += current_text\n",
    "            else:\n",
    "                cat_sec_name.append(current_sec)\n",
    "                cat_text.append(current_text)\n",
    "            current_text  = all_text[i]\n",
    "            current_token = tokens[i]\n",
    "            current_sec   = sec_name[i]\n",
    "            \n",
    "        if current_sec == sec_name[i] and current_token + tokens[i] <= max_token_soft:\n",
    "            current_text += \"\\n\"+all_text[i]\n",
    "            current_token += tokens[i]\n",
    "            \n",
    "        else:\n",
    "            cat_sec_name.append(current_sec)\n",
    "            cat_text.append(current_text)\n",
    "            current_text = all_text[i]\n",
    "            current_token = tokens[i]\n",
    "            current_sec = sec_name[i]\n",
    "    \n",
    "    \n",
    "    if current_text:\n",
    "        if current_token < min_token:\n",
    "            cat_text[-1] += current_text\n",
    "        else:\n",
    "            cat_text.append(current_text)\n",
    "            cat_sec_name.append(current_sec)\n",
    "\n",
    "    return cat_text, cat_sec_name\n",
    "\n",
    "import re\n",
    "def get_name(item_pool):\n",
    "    if 'ids' not in item_pool:return None\n",
    "    ids = item_pool['ids']\n",
    "    if ids['arxiv_id'] !=\"\":\n",
    "        return f\"[ArXiv:{ids['arxiv_id']}]\"\n",
    "    elif ids['doi'] !=\"\":\n",
    "        return f\"[DOI:{ids['doi']}]\"\n",
    "    elif ids['open_alex_id'] !=\"\":\n",
    "        return f\"[ALEX:{ids['open_alex_id'].split('/')[-1]}]\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def identify_string_type(s):\n",
    "    \"\"\"\n",
    "    give the unique know of a paper\n",
    "    \"\"\"\n",
    "    pattern1 = r\"^\\d+\\.\\d+$\"\n",
    "    pattern2 = r\"^[a-zA-Z]+\\/\\w+$\"\n",
    "\n",
    "    if re.match(pattern1, s):\n",
    "        return f\"ArXiv:{s}\"\n",
    "    elif re.match(pattern2, s):\n",
    "        return f\"ArXiv:{s}\"\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "    \n",
    "def process_a_paper(papers, index=0, encoding=None, use_bib=False):\n",
    "    \"\"\"This will change the dataframe `papers` inplace\"\"\"\n",
    "    metadata_i    = papers.loc[index, 'metadata']\n",
    "    body_text_i   = papers.loc[index, 'body_text']\n",
    "    bib_entries_i = papers.loc[index, 'bib_entries']\n",
    "    ref_entries_i = papers.loc[index, 'ref_entries']\n",
    "    \n",
    "    # get a dict for format citation/reference data\n",
    "    insert_data = {}\n",
    "    type_list = []\n",
    "    \n",
    "    # for formula, figure, table\n",
    "    figure_list =[]\n",
    "    table_list  =[]\n",
    "    paper_unique_id  = identify_string_type(metadata_i['id'])\n",
    "    for id, item in ref_entries_i.items():\n",
    "        type_list.append(item['type'])\n",
    "        key = \"{}:{}\".format(item['type'], id)\n",
    "        if item['type'] == 'formula':\n",
    "            value = \"\"\"${}$\"\"\".format(item['latex'])\n",
    "        elif item['type'] == 'table':\n",
    "            value = f\"\"\"[Table.{len(table_list)} of {paper_unique_id}]\"\"\"\n",
    "            table_list.append(item)\n",
    "        elif item['type'] == 'figure':\n",
    "            value = f\"\"\"[Figure.{len(table_list)} of {paper_unique_id}]\"\"\"\n",
    "            table_list.append(item)\n",
    "        else:\n",
    "            print('='*20)\n",
    "            print(item)\n",
    "            print('='*20)\n",
    "            value = \"\"\n",
    "        insert_data[key] = value\n",
    "        \n",
    "    # remove duplicate\n",
    "    type_list = list(set(type_list)) \n",
    "\n",
    "    # for citation\n",
    "    type_list.append('cite')\n",
    "    \n",
    "    for i,(_id, item) in enumerate(bib_entries_i.items()):\n",
    "        key = \"{}:{}\".format('cite', _id)\n",
    "        #if use_bib:\n",
    "        #    value = \"\"\"[Reference: {}]\"\"\".format(item['bib_entry_raw'])\n",
    "        #else:\n",
    "        #    value = \"[Reference]\"\n",
    "        value = get_name(item)\n",
    "        value = f\"[Ref.{i} of {paper_unique_id}]\" if value is None else value\n",
    "        insert_data[key] = value\n",
    "    \n",
    "    all_text = []\n",
    "    Reference= []\n",
    "    ReferenceQ= False\n",
    "    \n",
    "    for paragraph in body_text_i:\n",
    "        #print(paragraph.keys())\n",
    "        text = paragraph['text']\n",
    "        \n",
    "        start_string = text.strip()[:20]\n",
    "        if 'REFERENCES' in start_string or 'Reference' in start_string:\n",
    "            ReferenceQ=True\n",
    "            #print(start_string)\n",
    "        if 'Acknowledgement' in start_string:\n",
    "            #print(start_string)\n",
    "            continue\n",
    "        new_text = format_text_with_values(text, insert_data, type_list,paper_unique_id)\n",
    "        if len(new_text)==0:\n",
    "            continue\n",
    "        \n",
    "        if all_text and new_text.strip() and (new_text.strip()[0].islower()or new_text.strip()[:5] == 'Proof'):\n",
    "            all_text[-1]+=' '+new_text.strip()\n",
    "        elif all_text and all_text[-1].strip() and (all_text[-1].strip()[-1]==\"$\" or all_text[-1].strip()[-1]==\":\") and len(all_text[-1].split())<128:\n",
    "            all_text[-1]+=' '+new_text\n",
    "        else:\n",
    "            if ReferenceQ:\n",
    "                Reference.append(new_text.replace('\\n',\" \"))\n",
    "            else:\n",
    "                all_text.append(new_text.replace('\\n',\" \"))\n",
    "        paragraph['format_text'] = new_text\n",
    "    \n",
    "#     for t in all_text:\n",
    "#         print(len(t.split()))\n",
    "#         print(t+'\\n')\n",
    "#     raise\n",
    "    cat_final_i, sec_final_i = concatenate_text(all_text, body_text_i, metadata_i, encoding=encoding)\n",
    "    Reference = \"|.|\".join(Reference)\n",
    "    return all_text, cat_final_i, sec_final_i,Reference\n",
    "\n",
    "def process_papers(papers, encoding):\n",
    "    papers['all_text'] = None\n",
    "    papers['long_text_for_llm'] = None\n",
    "    papers['long_text_section'] = None\n",
    "    for i in tqdm.tqdm(range(len(papers)), leave=False, position=1):\n",
    "        all_text, cat_final_i, sec_final_i,Reference = process_a_paper(papers, i, encoding=encoding)\n",
    "        papers.at[i, 'all_text'] = all_text\n",
    "        papers.at[i, 'long_text_for_llm'] = cat_final_i\n",
    "        papers.at[i, 'long_text_section'] = sec_final_i\n",
    "        papers.at[i, 'Reference'] = Reference\n",
    "        \n",
    "    return papers\n",
    "\n",
    "def process_and_store(src_path, trg_path, encoding):\n",
    "    with open(src_path) as f:\n",
    "        papers = pd.read_json(path_or_buf=f, lines=True)\n",
    "    papers_new = process_papers(papers, encoding)\n",
    "\n",
    "    papers_new.to_json(trg_path, orient='records', lines=True)\n",
    "    \n",
    "    \n",
    "def convert_a_file(src_path):\n",
    "    # trg_path = src_path.replace(\"unarXive_230324_open_subset\", \"unarXive_230324_open_subset_new\")\n",
    "    trg_path = src_path.replace(\"unarXive_230324\", \"unarXive_clear_20230705\")\n",
    "    trg_dir = os.path.dirname(trg_path)\n",
    "    pathlib.Path(trg_dir).mkdir(exist_ok=True, parents=True)\n",
    "    if os.path.exists(trg_path):return\n",
    "    process_and_store(src_path, trg_path, encoding=encoding)\n",
    "    \n",
    "    \n",
    "def thread_func(all_files, start, end, total_length=5599, show=True, show_iters=5):\n",
    "    files_done = 0\n",
    "    with tqdm.tqdm(range(start, end), desc=\"Main\", ncols=100, ascii=True) as pbar:\n",
    "        for i in pbar:\n",
    "            file_name = all_files[i]\n",
    "            convert_a_file(file_name)\n",
    "            pbar.update(1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "829a0327",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting tiktoken\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/70/ea/a560501876b5fda9fc164224dbec6482bb31d5539728a5a2d69a4c7600e3/tiktoken-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /home/zhangtianning/.conda/envs/pytorch/lib/python3.9/site-packages (from tiktoken) (2023.3.23)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/zhangtianning/.conda/envs/pytorch/lib/python3.9/site-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/zhangtianning/.conda/envs/pytorch/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/zhangtianning/.conda/envs/pytorch/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/zhangtianning/.conda/envs/pytorch/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/zhangtianning/.conda/envs/pytorch/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab0d672",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b78756",
   "metadata": {},
   "source": [
    "### format abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed5836ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8845ddbb",
   "metadata": {
    "code_folding": [
     0,
     13,
     24
    ]
   },
   "outputs": [],
   "source": [
    "def identify_string_type(s):\n",
    "    \"\"\"\n",
    "    give the unique know of a paper\n",
    "    \"\"\"\n",
    "    pattern1 = r\"^\\d+\\.\\d+$\"\n",
    "    pattern2 = r\"^[a-zA-Z]+\\/\\w+$\"\n",
    "\n",
    "    if re.match(pattern1, s):\n",
    "        return f\"ArXiv:{s}\"\n",
    "    elif re.match(pattern2, s):\n",
    "        return f\"ArXiv:{s}\"\n",
    "    else:\n",
    "        return s\n",
    "def read_jsonl(path):\n",
    "    try:\n",
    "        with open(path,'r') as f:\n",
    "            data = [json.load(f)]\n",
    "    except json.JSONDecodeError:\n",
    "        with open(path,'r') as f:\n",
    "            data = []\n",
    "            for line in f:\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def structure_dict(_dict):\n",
    "    paper_id = _dict['paper_id']\n",
    "    abstract = _dict['abstract']\n",
    "    key_words= _dict['question']\n",
    "    pattern = r'\\d+\\.\\s*(.*?)(?=\\s*\\d+\\.\\s*|$)'\n",
    "    words   = re.findall(pattern, key_words)\n",
    "    if len(words)<1:\n",
    "        words = key_words.split(',')\n",
    "    return [paper_id] + [abstract] + words[:10]\n",
    "import os\n",
    "ROOTPATH='data/unarXive/split'\n",
    "all_json_list = []\n",
    "for level_1_name in os.listdir(ROOTPATH):\n",
    "    \n",
    "    level_1_path = os.path.join(ROOTPATH, level_1_name)\n",
    "    for level_2_name in os.listdir(level_1_path):\n",
    "        level_2_path = os.path.join(level_1_path, level_2_name)\n",
    "        all_json_list.append(level_2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "268e25a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ac0404f313409198cbfd6eca620fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def read_json_files(path):\n",
    "    return read_jsonl(path)\n",
    "\n",
    "\n",
    "all_data = []\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(16) as executor:\n",
    "    results = list(tqdm(executor.map(read_json_files, all_json_list), total=len(all_json_list)))\n",
    "\n",
    "for result in results:\n",
    "    all_data += result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "299f4866",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dois = []\n",
    "for i, d in enumerate(all_data):\n",
    "    dois.append([i,d['metadata']['doi']])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75387db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dois = [[a,b] for a,b in dois if b is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0a3ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "doismap = dict([[b,a] for a,b in dois])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "630b964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "eval_data = pd.read_csv(\"data/eval/RMP12.csv\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7225c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doi in eval_data['DOI']:\n",
    "    if doi in doismap:\n",
    "        print(doismap[doi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b201fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3afb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an HDF5 file and store the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e04ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ac64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "773d2624",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2979d509f04bfda7ab99ffccc8aa99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with h5py.File('dataset/unarXive.clear.title.h5', 'w') as f:\n",
    "    for row in tqdm(all_data):\n",
    "        paper_id    = 'title'\n",
    "        sentence_id = identify_string_type(row['metadata']['id'])\n",
    "        title    = row['metadata']['title'].replace('\\n', \" \").replace(\"  \", \" \").replace(\"  \", \" \").strip()\n",
    "        \n",
    "        # Create a group for each paper_id if it doesn't exist\n",
    "        if str(paper_id) not in f:\n",
    "            f.create_group(str(paper_id))\n",
    "\n",
    "        # Store the sentence as a dataset\n",
    "        f[f'{paper_id}'].create_dataset(str(sentence_id), data=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fcb44f",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "### format main content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8ae5afa",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "            Connecting The Dots To Combat Collective Fraud \n",
      "===========================\n",
      "Modern fraudsters write malicious programs to coordinate a group of accounts to commit collective fraud for illegal profits in online platforms. These programs have access to a set of finite resources - a set of IPs, devices, and accounts etc. and sometime manipulate fake accounts to collaboratively attack the target system. Inspired by these observations, we share our experience in building two real-time risk control systems to detect collective fraud. We show that with TigerGraph, a powerful graph database, and its innovative query language - GSQL, data scientists and fraud experts can conveniently implement and deploy an end-to-end risk control system as a graph database application.\n",
      "====================\n",
      "Detecting fraudulent activity is a never ending battle in the digital world. More and more merchants and financial organizations are targets for fraudsters and cybercriminals. Merchants and financial services organizations will spend $9.3 billion annually on fraud detection and prevention by 2022 (See [Ref.6 of ArXiv:2101.01898]). Global online payment fraud (also called CNP or “Card Not Present” fraud) alone will cost merchants $130 billion in just five years (from 2018 to 2023) (See [Ref.7 of ArXiv:2101.01898]). The latest report from LexisNexis (See [Ref.8 of ArXiv:2101.01898]) also indicates that fraud attempts have increased significantly among retailers and e-commerce merchants during the past year, with more than twice the number of attempts and an 85 percent increase in fraud success rates. \n",
      "\n",
      "A large class of fraud detection systems focus on detecting fraudulent accounts, as they are the main entities which if detected and blocked in time, can save a large amount of economic loss. \n",
      "\n",
      "The traditional way to combat account fraud is auditing (See [Ref.12 of ArXiv:2101.01898]), which maintains a black list of the reported bad accounts (or tag them based on some rules) and block them when they appear. However, online fraudsters are intelligent. They usually attack an online service by using a group of accounts to achieve scale and to bypass simple fraud detection rules. These accounts sneak in a system at different time and are usually camouflaged by sequences of seemingly innocuous behavior (random logins, browsing pages etc.). When the targeted campaign is launched, they work collaboratively to attack the system. This is the so-called collective fraud (See [Ref.5 of ArXiv:2101.01898]). \n",
      "\n",
      "Research on collective fraud has been focusing on devising different graph algorithms to mine account or transaction data to flag fraudulent entities (See [Ref.5 of ArXiv:2101.01898, ArXiv:1709.04129, Ref.4 of ArXiv:2101.01898]). Graph model is widely adopted for collective fraud detection due to the collaborative nature of the attack pattern. However, most of the systems in the literature either work on an existing heterogeneous network data set (See [Ref.5 of ArXiv:2101.01898, ArXiv:1709.04129]) or assume there is a dedicated existing account graph data model. Little is covered to address the practical hurdles emerged when building an end-to-end solution given the raw online activity logs. Such issues include but not limited to how to design and represent a suitable graph model, how to maintain and handle schema evolvement, how to setup and evolve data ingestion (a.k.a data ETL) pipeline, how to implement, fine tune and maintain the data analytic queries, how to activate the analytic query result to enable real-time risk control to prevent economic loss, and how to visually explain and further reasoning the detected fraudulent patterns. We point out that all these practical problems if handled in isolation without a standard data representation and programming language at different stages, it demands diverse background data engineers to collaborate across stages, adding non-trivial communication overhead and venues for errors. \n",
      "\n",
      "In this paper, we share our experience in building a real-time risk control system to prevent collective fraud end-to-end. We adopt TigerGraph , a native MPP graph database as our back-end server. The benefits of using a native graph database are unified data representation and programming API, data independency of application development and transparent query optimization. \n",
      "\n",
      "Section 2 and 3 presents the background information of TigerGraph architecture and GSQL. Section 4 describe our framework of the risk control system. Section 5 and 6 detail two applications. The paper is concluded in Section 6. \n",
      "\n",
      "TigerGraph’s architecture is depicted by Figure REF (in the blue boxes). The system vertical stack is structured in three layers: the top layer comprises the user programming interface, which includes the GSQL compiler, the GraphStudio visual SDK, and REST APIs for other languages to connect to TigerGraph; the middle layer contains the standard built-in and user defined functions (UDFs); the bottom layer includes the graph storage engine (GSE) and the graph processing engine (GPE). We elaborate on GSQL as it's the programming API to use TigerGraph. [Figure.0 of ArXiv:2101.01898]\n",
      "\n",
      "GSQL is a unified declarative graph database language for users to build applications end-to-end. It covers data modeling, data loading and data querying. \n",
      "\n",
      "DDL. GSQL’s DDL shares SQL’s philosophy of defining in the same CREATE statement a persistent data container as well as its type. This is in contrast to typical programming languages which define types as standalone entities, separate from their instantiations. \n",
      "\n",
      "GSQL’s CREATE statements can define vertex containers, edge containers, and graphs consisting of these containers. The attributes for the vertices/edges populating the container are declared using syntax borrowed from SQL’s CREATE TABLE command. In TigerGraph’s model, a graph may contain both directed and undirected edges. \n",
      "\n",
      "Listing shows an example of DDL, modeling an incentive campaign of a company. This campaign encourages its user to invite their friends to register an account. Bonus will be given to each account inviting new accounts. \n",
      "\n",
      "ACCUM, VERTEX, DIRECTED, UNDIRECTED, EDGE, CREATE, QUERY, GRAPH, WHILE, DO, END, TRUE, FALSE, OrAccum, MinAccum, HeapAccum, TYPEDEF, TUPLE, MapAccum, FLOAT, POSTACCUM, PRINT, LOG, FOREACH, SetAccum, MaxAccum, ListAccum, GroupByAccum, SumAccum, IF, DOUBLE, BagAccum, AvgAccum, TO, STRING, DATETIME, INT, USE, RUN, LOADING, JOB, USING, DEFINE, FILENAME, LOAD, USINGblue CREATE VERTEX Account (id STRING PRIMARY KEY, phone STRING, reg_time DATETIME) CREATE VERTEX IMEI (id STRING PRIMARY KEY, imei STRING) CREATE VERTEX Order (id STRING PRIMARY KEY, order_date DATETIME) CREATE UNDIRECTED EDGE use_imei (FROM Account, TO IMEI) CREATE DIRECTED EDGE invite (FROM Account, TO Account) CREATE DIRECTED EDGE send_bonus (FROM Account, TO Order) CREATE DIRECTED EDGE recv_bonus (FROM Order, TO Account) CREATE GRAPH IncentiveCampaignGraph (*) \n",
      "\n",
      "ACCUM, VERTEX, DIRECTED, UNDIRECTED, EDGE, CREATE, QUERY, GRAPH, WHILE, DO, END, TRUE, FALSE, OrAccum, MinAccum, HeapAccum, TYPEDEF, TUPLE, MapAccum, FLOAT, POSTACCUM, PRINT, LOG, FOREACH, SetAccum, MaxAccum, ListAccum, GroupByAccum, SumAccum, IF, DOUBLE, BagAccum, AvgAccum, TO, STRING, DATETIME, INT, USE, RUN, LOADING, JOB, USING, DEFINE, FILENAME, LOAD, USINGblue USE GRAPH IncentiveCampaignGraph CREATE LOADING JOB load_orders { DEFINE FILENAME jsonfile; LOAD jsonfile TO VERTEX BonusOrder VALUES($\"order_id\", \"order_date\"), TO EDGE send_bonus VALUES($\"sendr_phone\", $\"order_id\"), TO EDGE recv_bonus VALUES($\"order_id\", $\"recvr_phone\"), USING JSON_FILE=\"true\"; } CREATE LOADING JOB load_invite { DEFINE FILENAME invite_file; LOAD invite_file TO EDGE invite VALUES($0, $1); } RUN LOADING JOB load_orders USING jsonfile=\"/home/orders.json\" RUN LOADING JOB load_invite USING invite_file=\"/home/invite.csv\" \n",
      "\n",
      "DLL. GSQL provides a declarative loading language (DLL) to allow user to specify a loading job. The loading job can be invoked offline and online. The DLL is a mapping language. It comprises one or multiple LOAD statements. Each LOAD statement maps a source file schema to the targeted vertex or edge schema. The source file can be a Kafka source, or it can be raw file in JSON or CSV format. User references the source file column by its name or its column position in the VALUES clause. This is in resemblance of SQL INSERT statement. If column reference by position is used, the position numbering always starts from 0 and is represented by $0. \n",
      "\n",
      "For each loading job defined, a REST endpoint is created to allow online ingestion. For offline batch file loading, file binding happens at job invocation time, where \"RUN LOADING JOB\" command provides a USING clause to allow flexible run-time file binding. Listing shows an example of DLL. \n",
      "\n",
      "DML. GSQL DML is the core of the query language. The guiding principle behind the design of GSQL was to facilitate adoption by SQL programmers while simultaneously flattening the learning curve for novices, especially for adopters of the BSP programming paradigm (See [Ref.11 of ArXiv:2101.01898]). \n",
      "\n",
      "To this end, GSQL’s design starts from SQL, extending its syntax and semantics parsimoniously, i.e. avoiding the introduction of new keywords for concepts that already have an SQL counterpart. We summarize the key additional primitives. Please check (See [Ref.10 of ArXiv:2101.01898]) for more detailed description. \n",
      "\n",
      "Each GSQL query can be installed as a stored procedure and invoked via a REST endpoint. \n",
      "\n",
      " Graph Patterns in the FROM Clause. GSQL extends SQL’s FROM clause to allow the specification of patterns. Patterns specify constraints on paths in the graph, and they also contain variables, bound to vertices or edges of the matching paths. In the remaining query clauses, these variables are treated just like standard SQL tuple variables. Accumulators are special data types for GSQL developer to use. Local accumulator (prefixed by \"@\") serves as vertex's run-time attribute. Global accumulator (prefixed by \"@@\" stores the run-time query state. The aggregation results can be distributed across vertices, to support multi-pass and, in conjunction with loops, even iterative graph algorithms implemented in MPP fashion. Flow controls are supported in GSQL, in particular loops, which are essential to support standard iterative graph analytics (e.g. PageRank (See [ALEX:W2398953749]), shortest-paths (See [ALEX:W1574303240]), weakly connected components (See [ALEX:W1574303240]), recommender systems, etc.). [Figure.1 of ArXiv:2101.01898]\n",
      "\n",
      "We use an example to show GSQL. The left of Figure REF illustrates a pattern that we discovered from an online incentive campaign log. To achieve the networking effect of marketing, the merchant launched an incentive campaign to reward their existing users for inviting others to register. We notice this invitation chain is very long, more than 60 hops in depth, and each invitor in this chain invited exactly the same number of invitees, reflecting mechanic behavior. \n",
      "\n",
      "The invitation graph is a directed acyclic graph (DAG). We discovered this pattern by running a connected component (CC) algorithm coded using the GSQL query on the right of Figure REF . Line 1 declares a vertex runtime attribute (accumulator) named @ancestor. Its type is MaxAccum<vertex>, a builtin accumulator type, which has an initial value, and the user can keep accumulating (using its “+=” built-in combiner operator) new values into it. The name MaxAccum suggests this accumulator maintains the maximum vertex id it ever sees. Line 2-7 is the first query block, in the form of SELECT-FROM-WHERE-ACCUM structure. It matches all Account vertices (FROM clause) and finds those (WHERE clause) that has zero incoming edge and non-zero outgoing edges; these are CC ancestors. The ACCUM clause updates the ancestors’ run-time attribute @ancestor with its own id. \n",
      "\n",
      "After we find the CC ancestors, the second query block (Line 10-13) propagates the ancestors from the current vertex set to their direct children accounts. The edge pattern is specified in the FROM clause, where s and t are two bind variables at the source and the target of an edge, respectively. The ACCUM clause relays the source ancestor id to the target vertex. The SELECT clause assigns all the target vertices to a vertex set variable “children” (Line 10). The WHILE loop (Line 9) iterates the second query block until no more descendants are discovered. In the end, each vertex of a CC with size greater than 1 is adorned with their CC ancestor id in its @anecstor attribute. \n",
      "\n",
      "Once we find the CCs, we can profile them and report those that have long chains as anomalies. Or, we can calculate the CC depth while propagating its ancestors. \n",
      "\n",
      "This example showcases the conciseness and convenience of using GSQL . \n",
      "\n",
      "In this section, we give a high level overview of our risk control system architecture. \n",
      "\n",
      "Problem Description Given the account activity logs, the goal is to continuously detect collective fraud accounts, mark them with risk scores, and allow user to query their risk scores. [Figure.2 of ArXiv:2101.01898]\n",
      "\n",
      "An account is a logic concept. It represents a digital entity who is the first class citizen in an online service. Usually, each account is associated with one natural person. However, fraudsters are interested in using programs to manipulate a pool of fake or stolen accounts to simulate human behavior and obtain illegal profits. \n",
      "\n",
      "The whole architecture is shown in Figure REF . We use TigerGraph as the backend server to manage a dedicated account graph for fraud detection. The application designers first design an account graph model using GSQL's DDL. Next, the account activity logs are real-time bufferred in a Kafka cluster, and ingested into the account graph using a GSQL's DLL defined loading script. GSQL DML queries are used to code any fraud detection algorithms and point look-up query. All queries can be installed as a service and invoked via a REST endpoint. Via TigerGaphs's visual SDK, user can visually explore the subgraphs surrounding a risky account. \n",
      "\n",
      "Since the collaborative fraud accounts will form a connected component (CC) when they collaborate or share resources (IPs, Devices etc.), we adopt the following framework to detect collective fraud. \n",
      "\n",
      " An account graph is first constructed using either explicit or implicit relationships. Example, account-invite-account is an explicit relationship; an account shares an IP with another account in a time window is an implicit relationship. Next, a set of analytical GSQL queries (running periodically) will find account CCs and profile each CC using an aggregate statistic. The accounts in the CCs with anomalous statistics are flagged as fraudulent accounts. [Figure.3 of ArXiv:2101.01898]\n",
      "\n",
      "In this application, we explore the explicit relationships between two accounts and between accounts and their shared resources to construct our account graph. \n",
      "\n",
      "A company launched an incentive campaign, which encourages its user to invite their friends to open a new account with the company. Bonus will be given to the account inviting 10 new accounts. Soon after the campaign was launched, the company noticed that some accounts who won bonus sent their credits to one beneficiary account. This behavior indicates unusual user pattern. The goal here is to detect those accounts that are manipulated by fraudsters to illegally win the referral bonus. \n",
      "\n",
      "There are three logs: the bonus order log, the phone device log, and the referral log. The bonus order log schema is (order_id, order_date, bonus_name, sender_phone, recv_phone). The phone device log schema is (phone_number, imei). The referral log has the schema (recv_phone, recv_reg_date, sender_phone, sender_reg_date). \n",
      "\n",
      "In this graph construction, we use the explicit invite relationship to connect accounts. And we connect each account with their order and IMEI to explore their shared resource pattern. Specifically, we create the Account, the Order, the IMEI vertex types, the un-directed use_imei(connecting an Account and an IMEI), the directed invite(connecting two Accounts), and the directed send_bonus (connecting an Account and an Order) and recv_bonus (connecting an Account and an Order) edge types. \n",
      "\n",
      "We model and map the three logs to the graph schema using GSQL illustrated in Listing and Listing respectively. \n",
      "\n",
      "The analytic query we use is to cluster accounts into CCs using only the \"invite\" edges. After we find the account CCs, we profile each account CC with a statistic. Four CC profiling statistics are used in this application. \n",
      "\n",
      " CC Depth. In Fig. REF (a), the left shows a natural invitation graph, where an invitee has equal chance of inviting others or do not invite others. The right shows a machine manipulated pattern where accounts are re-used as much as possible, forming a long chain of invitees. Average Account Number Per Device. In Fig. REF (b), the left shows one IMEI usually identifies one account. The right shows fraudsters use one IMEI to register multiple accounts, which can be caught by the averaged accounts per IMEI of an account CC. Non-self-order Ratio. In Fig. REF (c), the left shows a normal behavior where an invitor sends the claimed bonus to itself. The right shows a fraudulent behavior where invitors collectively aggregates the claimed bonuses to one or multiple beneficiaries, which can be detected by the percentage of accounts in each CC that send their bonus to others. Gini index. The Gini index (See [Ref.13 of ArXiv:2101.01898]) measures the inequality among values of a frequency distribution. We use the Gini index of each account invited guest number to capture collective fraud. Fig. REF (d) shows why fraudsters will likely form a homogeneous invitation chain. Three scenarios all use 10 accounts each. Assuming 3 invitees can help their invitor win 1 bonus. To maximize the claimed bonus, (d) shows three strategies. Strategy 1 wins 2 bonus. Strategy 2 and strategy 3 win 3 bonus each. However, strategy 2 is easily to be blocked if the system requires that one account at most claims 1 bonus. Strategy 3 not only by pass the aforementioned simple blocking rule, it also maximizes the utilization of the 10 accounts. \n",
      "\n",
      "Goal. Our goal is to answer two questions. \n",
      "\n",
      " What are the precision and recall using this method? Each CC statistics targets a hypothetical collective fraud pattern, do these patterns exist in a real data? [Figure.8 of ArXiv:2101.01898]\n",
      "\n",
      "Setup. This is a proof of concept (PoC) project conducted for our client using the post-campaign data. We applied the system over 1 month real data after the campaign was launched. We computed the CC using the \"invite\" edge only, and finally used the 4 CC statistics to detect anomalies. \n",
      "\n",
      "Table REF shows the vertex and edge statistics for each type. \n",
      "\n",
      "Figure REF shows the invitation graph's statistics. In general, the account out-degree histogram (the left chart) follows the power-law with the exception that there is a spike in the bucket positioned at the out-degree value 10. The campaign manager shared with us that this campaign has set the invitees threshold number to be 10 for an invitor to win a referral bonus, which explains the spike value. The account distribution in different CC size histogram (the right chart) shows that more than half of the accounts are in a CC whose size is between 10 and 100. [Table.21 of ArXiv:2101.01898]\n",
      "\n",
      "We wrote a GSQL DDL script define the schema of the invite graph. Another GSQL DLL script was defined to load the three logs into the invite graph. The four CC profiling algorithms were written in GSQL DML, and extending the basic DAG CC detection algorithm depicted in Figure REF . For each CC profiling algorithm, we pick a very conservative threshold parameter to detect anomaly. These threshold are specified in the Description column of Table REF . All the GSQL scripts can be accessed onlineIncentive Fraud GSQL scripts: http://github.com/beader/combat_collective_fraud.. \n",
      "\n",
      "To evaluate the precision and recall of our method, we need to know which accounts are true fraud accounts. We rely on Tencent T-Sec product, which is an anti-fraud cloud service to provide the risk score for each phone number. We rely on T-Sec to flag our test data since Tencent has billions of monthly active users and more data to profile each user. \n",
      "\n",
      "Result. Table REF presents the results. (1) This campaign gave out $112,798 incentive bonus. Conservatively estimate, 33346 accounts are fraudsters controlled, they collectively claimed 6000 bonus. Each bonus claim costs $7 loss to our client, total loss added up to $42,299.40. (2) Exclude Gini index method (which was calculated based on each invitor's invitee count in a CC), still we have 25,760 fraudulent accounts, which claimed 5071 incentive bonus orders, amounting to $40,184. (3) When calculate anomalous accounts based on e and f, we do not use CC. They are one step statistics of an account. We notice the discovered accounts is significantly less than using the CC methods. From calling Tecent APIs, we compute that the combined methods achieved the precision of 94.7% and the recall of 81.9%. \n",
      "\n",
      "Further, we show some examples of the verified fraudulent CCs in Figure REF . They are explainable following our initial CC statistics design hypothesis.(a) and (b) are self-explanatory. In (c) we show a CC that all the bonus claims were sent to others. The pink vertices represent a claim transaction, which connects the sender and receiver phone. It shows some claims were sent to one account that does not belong to the CC (the central one), and some claims were aggregate to some account within the CC (the left one). In (d), we show a low Gini index CC, where each phone invited 10 phones, 9 surround the invitor, and 1 forms a new invitee cluster. \n",
      "\n",
      "Based on the positive result of the PoC, the client plans to deploy the detection framework for their next campaign, running CC profiling periodically, and raise alarm when discovering CC that outnumber the pre-set cutoff. [Figure.9 of ArXiv:2101.01898][Figure.10 of ArXiv:2101.01898]\n",
      "\n",
      "In this application, we explore the implicit relationships between accounts and their shared resource. We devise a time-sensitive co-context edge construction technique to construct the account graph. \n",
      "\n",
      "One e-commerce website wants to setup a risk control system, which will be serving critical business steps to decide whether their current user's action post high risk to the business. The input to the risk control system would be the real-time risk control log, the entry of which is a risk control event. An event log usually contains event timestamp, type, event context (IP, location etc.) and an account. No explicit relationship between accounts exists in the event log. \n",
      "\n",
      "For each new risk control event, the system will digest the event information and update the risk scores for the affected accounts. Subsequently, any applications relying on the risk control system can real-time get the risk score for their current user, and take action when high risk score is returned. \n",
      "\n",
      "In this graph construction, the input data does not contain explicit relationships between accounts. To connect accounts, we use co-context edge construction technique to explore the share resource behaviors of the fraudulent accounts. \n",
      "\n",
      "We first describe the co-context edge concept and then show how we apply it to our current data set. \n",
      "\n",
      "Co-context Edge. How do we connect accounts from an account activity log when there is no explicit relationship? The answer is to use co-context. A co-context abstracts a shared resource as a context, and accounts sharing the same context will be connected. \n",
      "\n",
      "For example, an account event log contains the device id used, the account and the IP information. One way to construct a graph from the log entry is shown on the left of Figure REF , where we create a vertex for each account, device and IP respectively, and connect an account and an IP, and connect an account and a Device when they appear in the same log entry. There are certain drawbacks of this approach. (1) It does not handle dynamic IP assignment issue, since an IP can be mixed used by both good and bad accounts. (2) It needs two hops from one account to reach the other account: one hop is from the account to IP, the other hop is from the IP to another account. This added hop incurs computational overhead. (3) It is hard to define the strength between two accounts over the two-hop model. \n",
      "\n",
      "We can use co-context concept to connect accounts. In this modeling approach, we collapse the above two-hop into one context edge. For shared resource entities that co-occur with a pair of accounts, we create edges to represent each shared resource. On the right graph of Figure REF , when two accounts share the same IP, we create an edge to connect them. The blue edges between a and b, b and c, and c and d indicate each pair shares the same IP. The pink edges between a and c, and b and d indicate that each pair shares the same device. We call this modeling method co-context edge modeling method. It has the following advantages:  \n",
      "Connecting account vertices directly by the contextual edge, it is easier to define the link strength between the end points. It is easier to merge edges between accounts. We can summarize multiple co-context edges into one since they share the same end points. Another advantage is that we just need 1-hop to traverse from one account to another, achieving the best performance. [Figure.15 of ArXiv:2101.01898]\n",
      "\n",
      "Time Constraint and Timestamp of Co-context Edge. Fraudsters manipulate a common set of resources (IPs, Devices etc.). Their generated events show vicinity along the time dimension when they re-use the shared resources. Seeing this, we can add a time window constraint to further reduce the generated co-context edges. Imagine the following, an IP address was used at 9:00am by user u1, and used again by user u2, and u3 at 15:00pm and 15:01pm respectively. Then, the IP edge strength between u1 and u2 is weaker than the IP edge strength between u2 and u3. \n",
      "\n",
      "To construct time-sensitive co-context edge, we can add a time window while sweeping the event log. For example, we can set the window size to be 30 seconds. In the aforementioned example, (u2, u3)'s time stamp difference is less than 30 seconds, they will form a co-context edge; we use the latter timestamp of the two endpoints as the edge's creation timestamp. In the (u2, u3)'s case, the created edge has timestamp 15:01pm. On the other hand, (u1, u2)'s will not form a co-context edge as they do not fall within a time window. \n",
      "\n",
      "Figure REF shows more examples that we can use this modeling approach to create edges. [Figure.16 of ArXiv:2101.01898]\n",
      "\n",
      "Applying Co-context Edge. In this application, each log entry has the account, the IP it uses, and the event type and timestamp. We create the account graph using the co-IP context. The graph has an Account vertex type, and a un-directed shared_ip edge type, connecting two Account vertices when they both use the same IP within a 30-second time window. \n",
      "\n",
      "When constructing the shared_ip edges, instead of creating an edge per pair of accounts in the same time window, we only create an edge for time-adjacent account pairs. This implementation greatly reduces the total edge count, while still preserves the important distance information between any two accounts from a CC. It is illustrated in Fig REF . \n",
      "\n",
      "The analytic query we use is to cluster accounts into CCs using the shared_ip co-context edges. After we find the account CCs, we profile each account CC using the CC size statistic. We record in each account as its properties the cc size and the update timestamp. These two materialized properties are used to answer real-time risk control request. \n",
      "\n",
      "Using the CC size as the profiling statistics deserves further discussion. When fraudsters' programs manipulate accounts using their limited IPs, to maximize their profits of the targeted campaigns, they tends to maximize the amount of their account activities with the shared IP. It's very unlikely two distinct normal accounts of a website will share the same IP as the IPs are usually assigned according to the same geolocation or ISP. Even when two normal accounts do share an IP, the chances that they both visit the website at a small time window is very slim. As a result, with a small time window as the shared_ip creation criteria, we can effectively capture the fraud account clusters. \n",
      "\n",
      "There do exist scenarios where this method may not work. For example, for a national holiday such as Thanksgiving or Christmas, peoples living in the same neighborhood are more likely to visit the same website for online shopping on the same day. In such scenario, two normal accounts may share an IP within a small time window. \n",
      "\n",
      "On the other hand, suppose the fraudsters buy some commercial proxy services to obtain a new IP per their account use to bypass this method, it will incur non-trivial cost for the fraudsters. Increasing the cost of fraudsters attack is another way to deter their efforts. \n",
      "\n",
      "Goal. Our goal is to answer the following questions before deploying the risk control system to production. \n",
      "\n",
      " Is this method scalable in terms of storage and performance with respect to the size of the risk control log? Can this system provide high QPS to serve as an operational risk control service 24x7? How are the precision and recall affected by the con-context time window size? What are the precision and recall? With what parameters setting? \n",
      "\n",
      "Setup. In this experiment, we use the e-commerce website’s 10-day risk control log from 2019-12-01 to 2019-12-09. The log contains 4344376 raw account events, and about 1.2 million accounts. The Tencent's T-Sec API flagged 7.6k accounts as high risk accounts, which serves as our canonical truth when computing precison and recall. \n",
      "\n",
      "For all the experiments, we use a server with Intel(R) Core(TM) i7-7800X CPU@3.50GHz, 12 Cores and 64G RAM. \n",
      "\n",
      "To test the scalability of this method, we used the full 10 days data, and varied the window size of constructing co-context edges from 10s to 3600s. We varied the window size since with bigger co-context edge construction windows size, for the same input data, we will have bigger account graph, which helps us to test our system scalability with regard of account graph size. ßWe record the vertex and edge counts with respect to the window size. On the account graphs of different window size, we record the elapsed time to do one pass of CC update. \n",
      "\n",
      "To test the QPS, we ran two experiments over the full 10 days data, and fixed the window size to 3600s to stress test the worst situation. The first experiment tests the throughput of CC size lookup query by repeatedly sending concurrent REST requests to the graph database with no CC detection query running in the background. The second experiment tests the same lookup query QPS, but with the CC detection and account CC size update query in the background 24x7. We reported the QPS of each. \n",
      "\n",
      "To measure the time window size's influence on the algorithm accuracy, we compute the precision, recall and F score for each window size over the same 10-day log. \n",
      "\n",
      "To compute the precision and recall, we set the time window size to 30s, and the CC size threshold to 10. We show the charts with different values of the two parameters to explain why we pick them. The constructed account graph has 95,827 accounts, connected by 92,413 shared_ip edges, excluding those accounts that has no edges. We run the un-directed CC detection algorithm and update the CC size of each account. \n",
      "\n",
      "Result. [Figure.17 of ArXiv:2101.01898]\n",
      "\n",
      "In Fig. REF , the left chart shows that as the window size increases, the total account number in the account graph is going up sub-linearly, while the total number of co-context edge is going up linearly. Usually, as the vertex number goes up, the edge number exponentially goes up. In our implementation, since we only constuct co-context edge with time-adjacent accounts, we achieve linear scalability of the edge count, which resulted linear storage scalability of the size of the constructed account graph. In Fig. REF , the right chart shows the CC computation time v.s. each account graph's (created by different window size) vertex and edge count. The elapsed time to compute CC for the entir graph is linear with respect to the graph element size, showing good performance scalability of this method. [Table.28 of ArXiv:2101.01898]\n",
      "\n",
      "In Table REF , we show the 95 percentile of the QPS benchmark with or without the CC algorithm running in the background. Without the CC algorithm running in the background, 95% of the CC size lookup query can finish within 25ms. With the CC algorithm running, we achieve 31ms upper limit for the 95% of query. This shows excellent real-time response performance, which is critical for risk control system, since the sooner it discovers fraudulent account, the easier for the client to save the economic loss of the attack. The two QPS 14.57k v.s. 11.89k are both above most of the risk control system requirement. For our client, they expect 5k will be sufficient for their business operation. \n",
      "\n",
      "Finally, we discuss the relationship between parameters and precision and recall. In Fig. REF , the left chart shows with larger window size to construct the shared_ip edge, the system's precision drops significantly. Good precision and recall are achieved when the window size is less than 100s. The reason the false positive increases is that with a larger window, the likelihood of two normal account share the same IP is increased, thus causing many false alarms. In Fig. REF , the right chart shows the CC size histograms of different window sizes. The x-axis is the CC size, and the y-axis is the CC count. The red line marks the CC size of 10. For this data set, we can pick any CC size threshold from 10 to 100 without changing the fraud detection rate. We pick 10 as our final production parameter as it provides a good trade-off between precision and recall, and the sensitivity of our risk control system. The real deployment can fine tune this parameter to adapt to the production usage pattern. \n",
      "\n",
      " Precision and Recall: Scalability: Choosing the CC size threshold: QPS. \n",
      "\n",
      "The framework reports 70,086 accounts as risky accounts. Among them, 69532 also flagged as risky by the cloud service A. Note that the service A flagged total 7.6k accounts as risky accounts. The 6k accounts missed by the co-context graph is by design, as they are not part of the constructed graph. Even without having the opportunity to score the missed 6k accounts, the account graph has a detection rate of 91.4% (69532/7.6k), which is pretty good. \n",
      "\n",
      "After the succesful PoC, our client has deployed the delivered system to production since April 2020. In the deployment, when construction shared_ip edges, we used the window size of 3600s. On each edge, we store its creation time and the time delta of the two account events it connects. When computing CC, our GSQL query filters those edges that are created beyond the latest 7 days. Also, by filtering edges based on its time delta property, we can apply any window size from 1s to 3600s to our production system. Since GSQL is installed as a stored procedure and invoke by REST end point, we can dynamically tune our algorithms with the window size to adapt to the real log stream changes. [Figure.18 of ArXiv:2101.01898]\n",
      "\n",
      "PathSim (See [Ref.9 of ArXiv:2101.01898]) explores the MetaPath on existing static heterogeneous network to compute the similarity of two nodes. Our co-context edge is real-time constructed for a dedicate risk control account graph. HitFraud (See [ArXiv:1709.04129]) conducted semi-supervised learning by extracting graph features from the existing transaction graph connected by MetaPath(See [Ref.9 of ArXiv:2101.01898]), and these features are used with other labeled data to train a transaction classifier. Our approach is unsupervised and builds a dedicate dynamic account graph from the log, and only add to the graph the shared resources vertices. We study the machine behavior of account CCs, not individual account. HGsuspector (See [Ref.5 of ArXiv:2101.01898]) decomposes directed offline property graph into a set of bi-party graphs, account - x (ip, device, etc.) , then it scores each connected bi-party graph, and report anomaly if a threshold is met. The drawback is that it is not explainable and it passively takes an existing graph. Our approach regards constructing a dedicate account graph in real-time as the first step is important, and only adding risk-control relevant edges between accounts. The framework explores the program mechanic pattern and shared resource pattern on the account CCs, and the results are explainable. \n",
      "\n",
      "We have presented two risk control systems to detect collective fraud. Both system are implemented by GSQL and powered by TigerGraph database. We adopt the approach to build a dedicate account graph to detect fraud. This is in contrast to detect fraud on existing heterogeneous network data, where many intermediate links and vertices existed between accounts. The account graph explores the machine behavior and the finite shared resource characteristic by profiling account CCs. Both applications demonstrate the practical advantage of building a risk control system using a graph database. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "ROOTPATH='dataset/unarXive_clear_20230705/'\n",
    "all_json_list = []\n",
    "for level_1_name in os.listdir(ROOTPATH):\n",
    "    level_1_path = os.path.join(ROOTPATH, level_1_name)\n",
    "    for level_2_name in os.listdir(level_1_path):\n",
    "        level_2_path = os.path.join(level_1_path, level_2_name)\n",
    "        all_json_list.append(level_2_path)\n",
    "with open(all_json_list[73]) as f:\n",
    "    papers = pd.read_json(path_or_buf=f, lines=True)\n",
    "index    = 0\n",
    "result   = process_a_paper(papers,index)\n",
    "metadata = papers.iloc[index].metadata\n",
    "title    =  metadata['title'].replace('\\n',' ')\n",
    "print(f\"\"\"===========================\n",
    "            {title} \n",
    "===========================\"\"\")\n",
    "print(metadata['abstract'].strip().replace('\\n',\" \"))\n",
    "print(\"=\"*20)\n",
    "for t in result[0]:\n",
    "    print(t+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15acdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5a1d82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10727894"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4abf890",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def write_row_to_hdf5(row, hdf5_file):\n",
    "    with h5py.File(hdf5_file, 'a') as f:\n",
    "        paper_id    = row['paper_id']\n",
    "        sentence_id = row['section_num']\n",
    "        sentence    = row['section']\n",
    "        \n",
    "        # Create a group for each paper_id if it doesn't exist\n",
    "        if str(paper_id) not in f:\n",
    "            f.create_group(str(paper_id))\n",
    "\n",
    "        # Store the sentence as a dataset\n",
    "        f[f'{paper_id}'].create_dataset(str(sentence_id), data=sentence)\n",
    "\n",
    "# Create an HDF5 file and store the data\n",
    "hdf5_file = 'dataset/unarXive.clear.sections.h5'\n",
    "\n",
    "# Initialize the HDF5 file\n",
    "with h5py.File(hdf5_file, 'w'):\n",
    "    pass\n",
    "\n",
    "# Use multiprocessing to parallelize the writing process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa5436b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bee021fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_jsonl(all_json_list[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9b175924",
   "metadata": {
    "code_folding": [
     0,
     11
    ]
   },
   "outputs": [],
   "source": [
    "def read_jsonl(path):\n",
    "    try:\n",
    "        with open(path,'r') as f:\n",
    "            data = [json.load(f)]\n",
    "    except json.JSONDecodeError:\n",
    "        with open(path,'r') as f:\n",
    "            data = []\n",
    "            for line in f:\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "def structure_dict(_dict):\n",
    "    paper_id = _dict['paper_id']\n",
    "    abstract = _dict['abstract']\n",
    "    key_words= _dict['question']\n",
    "    pattern = r'\\d+\\.\\s*(.*?)(?=\\s*\\d+\\.\\s*|$)'\n",
    "    words   = re.findall(pattern, key_words)\n",
    "    if len(words)<1:\n",
    "        words = key_words.split(',')\n",
    "    return [paper_id] + [abstract] + words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee2ada65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1f116e5",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd3fce476ce4376bd21b6a2edd99b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_data = [] \n",
    "for path in tqdm(all_json_list):\n",
    "    all_data+=read_jsonl(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "30238885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9209c4bb343e88ab805ed0007149eddc0f2f842c\n",
      "09f7d8e302939677061e718778e2ebf9e19fdb22\n",
      "b62c466cacc990508dbbb34e5c15681be91cf252\n",
      "9003f7c17f3de907d848f52ec6ff19fc43cb69c5\n",
      "9555e14b8722bde3a8379bb02f6f936074b29748\n",
      "2b4b0671764aa55c2b99d86f2c03d69f75f0a39c\n",
      "5c6192d1af2a4eebb5b6f7f67b75c8edb799c749\n",
      "487d1c1bcc95f533a0b46616aaf3f4acc954db7b\n",
      "04dc09d45391dd39dc88899fa1416ac33387e9de\n",
      "d60fe6060c930c93a875dddae6d75e43434d1fee\n",
      "948f6a6ab7a7048c510baf1914e6791f2fdbbd2d\n",
      "ef5442955c843be18174385155cf6fde5535c7e1\n",
      "44a90f86c67019fefd4b5f779c742f2e719739d4\n",
      "59dfdf31f249c50692d92c63b95423ccd0275b18\n",
      "6f93b75b5b4b84abfaee5ff2fa4804904ab2e7da\n",
      "c0d112191c8d160f7260384e5f8f24238d934202\n",
      "1bb8f31715f1a294eece336bee15d868f2f55f04\n"
     ]
    }
   ],
   "source": [
    "for t in all_data[0]['bib_entries']:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "98db5161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([493]),)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(aaa==aaa.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dd05271",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allquesion = np.array([structure_dict(d) for d in  all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c944eba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allquesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ec009fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7d15557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rowstats = [len(t) for t in allquesion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9378f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "badslots = np.where(np.array(rowstats)<12)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "db73e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data = allquesion[np.array(rowstats)==12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "638bb23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_titles = ['paper_id', 'abstract', 'key_word_1', 'key_word_2', 'key_word_3',\n",
    "                 'key_word_4', 'key_word_5', 'key_word_6', 'key_word_7', 'key_word_8',\n",
    "                 'key_word_9', 'key_word_10']\n",
    "df = pd.DataFrame(good_data.tolist(), columns=column_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "563e9c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('unArXiv.key_word_from_abstract.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lcoal",
   "language": "python",
   "name": "lcoal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
