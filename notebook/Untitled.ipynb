{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1d98b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9d14bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a63de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.argsort(a, descending=True)\n",
    "#c = torch.argsort(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16ed24a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.8198e+00,  3.0566e+00,  2.8545e+00,  2.5881e+00,  2.5787e+00,\n",
       "         2.5631e+00,  2.5088e+00,  2.4963e+00,  2.4103e+00,  2.3860e+00,\n",
       "         2.3393e+00,  2.3253e+00,  2.2364e+00,  2.2192e+00,  2.1951e+00,\n",
       "         2.1190e+00,  2.0999e+00,  2.0897e+00,  2.0714e+00,  2.0468e+00,\n",
       "         2.0348e+00,  1.9845e+00,  1.9514e+00,  1.9166e+00,  1.9151e+00,\n",
       "         1.9022e+00,  1.9000e+00,  1.8922e+00,  1.8868e+00,  1.8695e+00,\n",
       "         1.8613e+00,  1.8564e+00,  1.8105e+00,  1.8064e+00,  1.7909e+00,\n",
       "         1.7682e+00,  1.7160e+00,  1.7093e+00,  1.6823e+00,  1.6813e+00,\n",
       "         1.6783e+00,  1.6603e+00,  1.6411e+00,  1.6348e+00,  1.6297e+00,\n",
       "         1.6216e+00,  1.6116e+00,  1.6113e+00,  1.6052e+00,  1.5980e+00,\n",
       "         1.5975e+00,  1.5952e+00,  1.5914e+00,  1.5903e+00,  1.5476e+00,\n",
       "         1.5344e+00,  1.5272e+00,  1.5233e+00,  1.5208e+00,  1.5177e+00,\n",
       "         1.5145e+00,  1.5108e+00,  1.5091e+00,  1.4893e+00,  1.4770e+00,\n",
       "         1.4723e+00,  1.4678e+00,  1.4665e+00,  1.4649e+00,  1.4572e+00,\n",
       "         1.4566e+00,  1.4502e+00,  1.4481e+00,  1.4459e+00,  1.4452e+00,\n",
       "         1.4385e+00,  1.4351e+00,  1.4230e+00,  1.4092e+00,  1.4054e+00,\n",
       "         1.4047e+00,  1.4031e+00,  1.3962e+00,  1.3823e+00,  1.3810e+00,\n",
       "         1.3757e+00,  1.3618e+00,  1.3508e+00,  1.3382e+00,  1.3218e+00,\n",
       "         1.3034e+00,  1.3010e+00,  1.2995e+00,  1.2826e+00,  1.2825e+00,\n",
       "         1.2705e+00,  1.2705e+00,  1.2685e+00,  1.2662e+00,  1.2608e+00,\n",
       "         1.2445e+00,  1.2436e+00,  1.2425e+00,  1.2403e+00,  1.2339e+00,\n",
       "         1.2250e+00,  1.2129e+00,  1.2013e+00,  1.2010e+00,  1.1866e+00,\n",
       "         1.1861e+00,  1.1856e+00,  1.1798e+00,  1.1711e+00,  1.1683e+00,\n",
       "         1.1565e+00,  1.1560e+00,  1.1543e+00,  1.1542e+00,  1.1465e+00,\n",
       "         1.1464e+00,  1.1428e+00,  1.1396e+00,  1.1331e+00,  1.1221e+00,\n",
       "         1.1177e+00,  1.1167e+00,  1.1128e+00,  1.1075e+00,  1.0952e+00,\n",
       "         1.0937e+00,  1.0863e+00,  1.0838e+00,  1.0753e+00,  1.0717e+00,\n",
       "         1.0706e+00,  1.0670e+00,  1.0656e+00,  1.0638e+00,  1.0520e+00,\n",
       "         1.0520e+00,  1.0511e+00,  1.0480e+00,  1.0462e+00,  1.0456e+00,\n",
       "         1.0412e+00,  1.0404e+00,  1.0331e+00,  1.0282e+00,  1.0280e+00,\n",
       "         1.0270e+00,  1.0248e+00,  1.0245e+00,  1.0234e+00,  1.0216e+00,\n",
       "         1.0206e+00,  1.0198e+00,  1.0188e+00,  1.0179e+00,  1.0023e+00,\n",
       "         1.0019e+00,  9.9325e-01,  9.9178e-01,  9.8944e-01,  9.8784e-01,\n",
       "         9.8471e-01,  9.7996e-01,  9.7632e-01,  9.7591e-01,  9.6771e-01,\n",
       "         9.6548e-01,  9.6283e-01,  9.6212e-01,  9.4989e-01,  9.4425e-01,\n",
       "         9.3846e-01,  9.3232e-01,  9.2854e-01,  9.2769e-01,  9.2378e-01,\n",
       "         9.2226e-01,  9.1932e-01,  9.1547e-01,  9.1510e-01,  9.1484e-01,\n",
       "         9.1084e-01,  9.0615e-01,  9.0564e-01,  9.0461e-01,  8.9972e-01,\n",
       "         8.9677e-01,  8.9657e-01,  8.9482e-01,  8.9352e-01,  8.9011e-01,\n",
       "         8.8952e-01,  8.8375e-01,  8.8341e-01,  8.8212e-01,  8.8173e-01,\n",
       "         8.8144e-01,  8.7947e-01,  8.7762e-01,  8.7742e-01,  8.6827e-01,\n",
       "         8.6189e-01,  8.4682e-01,  8.4371e-01,  8.3957e-01,  8.2679e-01,\n",
       "         8.2449e-01,  8.1677e-01,  8.1630e-01,  8.1392e-01,  8.0451e-01,\n",
       "         8.0082e-01,  7.9879e-01,  7.9400e-01,  7.8245e-01,  7.7876e-01,\n",
       "         7.7536e-01,  7.6143e-01,  7.6029e-01,  7.5984e-01,  7.5870e-01,\n",
       "         7.5377e-01,  7.4145e-01,  7.3828e-01,  7.3737e-01,  7.3307e-01,\n",
       "         7.3185e-01,  7.2879e-01,  7.2784e-01,  7.2688e-01,  7.2020e-01,\n",
       "         7.1555e-01,  7.1341e-01,  7.0993e-01,  7.0983e-01,  7.0107e-01,\n",
       "         6.9982e-01,  6.9952e-01,  6.9671e-01,  6.9479e-01,  6.9271e-01,\n",
       "         6.9271e-01,  6.8874e-01,  6.8634e-01,  6.8370e-01,  6.8125e-01,\n",
       "         6.8088e-01,  6.8057e-01,  6.7599e-01,  6.7520e-01,  6.7225e-01,\n",
       "         6.6977e-01,  6.6942e-01,  6.6289e-01,  6.6242e-01,  6.5825e-01,\n",
       "         6.5559e-01,  6.5434e-01,  6.5222e-01,  6.5180e-01,  6.4672e-01,\n",
       "         6.4293e-01,  6.3835e-01,  6.3724e-01,  6.3533e-01,  6.2898e-01,\n",
       "         6.2790e-01,  6.2605e-01,  6.2341e-01,  6.1248e-01,  6.1196e-01,\n",
       "         6.1142e-01,  6.0252e-01,  5.9981e-01,  5.9513e-01,  5.8891e-01,\n",
       "         5.8293e-01,  5.7872e-01,  5.7374e-01,  5.6640e-01,  5.6180e-01,\n",
       "         5.6018e-01,  5.5533e-01,  5.5269e-01,  5.4864e-01,  5.4307e-01,\n",
       "         5.4111e-01,  5.3912e-01,  5.3871e-01,  5.3405e-01,  5.2795e-01,\n",
       "         5.2473e-01,  5.1948e-01,  5.1758e-01,  5.1660e-01,  5.1467e-01,\n",
       "         5.1424e-01,  5.1265e-01,  5.1029e-01,  4.9841e-01,  4.9300e-01,\n",
       "         4.8975e-01,  4.8951e-01,  4.8742e-01,  4.8349e-01,  4.8173e-01,\n",
       "         4.7873e-01,  4.7597e-01,  4.7452e-01,  4.7273e-01,  4.6411e-01,\n",
       "         4.6316e-01,  4.6028e-01,  4.5661e-01,  4.5254e-01,  4.5210e-01,\n",
       "         4.4888e-01,  4.4742e-01,  4.4561e-01,  4.4339e-01,  4.3764e-01,\n",
       "         4.3054e-01,  4.2949e-01,  4.2886e-01,  4.2876e-01,  4.2806e-01,\n",
       "         4.2106e-01,  4.2063e-01,  4.1980e-01,  4.1569e-01,  4.1431e-01,\n",
       "         4.1130e-01,  4.0874e-01,  4.0710e-01,  4.0370e-01,  4.0263e-01,\n",
       "         3.9837e-01,  3.9836e-01,  3.9707e-01,  3.9526e-01,  3.9486e-01,\n",
       "         3.9480e-01,  3.8850e-01,  3.8680e-01,  3.8649e-01,  3.8141e-01,\n",
       "         3.7746e-01,  3.7707e-01,  3.7223e-01,  3.7085e-01,  3.6968e-01,\n",
       "         3.6592e-01,  3.6187e-01,  3.6143e-01,  3.5824e-01,  3.5762e-01,\n",
       "         3.5755e-01,  3.5637e-01,  3.5600e-01,  3.5029e-01,  3.4963e-01,\n",
       "         3.4892e-01,  3.4197e-01,  3.3784e-01,  3.3250e-01,  3.3235e-01,\n",
       "         3.3048e-01,  3.2823e-01,  3.1851e-01,  3.1310e-01,  3.0996e-01,\n",
       "         3.0722e-01,  3.0592e-01,  3.0349e-01,  3.0327e-01,  3.0081e-01,\n",
       "         2.9817e-01,  2.9638e-01,  2.8802e-01,  2.8792e-01,  2.8663e-01,\n",
       "         2.8642e-01,  2.8373e-01,  2.8277e-01,  2.7953e-01,  2.7901e-01,\n",
       "         2.7325e-01,  2.6960e-01,  2.6243e-01,  2.6036e-01,  2.5967e-01,\n",
       "         2.5766e-01,  2.5191e-01,  2.4976e-01,  2.3138e-01,  2.3093e-01,\n",
       "         2.2812e-01,  2.2418e-01,  2.2305e-01,  2.2148e-01,  2.1934e-01,\n",
       "         2.1805e-01,  2.1447e-01,  2.1430e-01,  2.1400e-01,  2.1372e-01,\n",
       "         2.1151e-01,  2.1042e-01,  2.0645e-01,  2.0236e-01,  1.9723e-01,\n",
       "         1.9545e-01,  1.9449e-01,  1.9233e-01,  1.9139e-01,  1.8200e-01,\n",
       "         1.8150e-01,  1.7759e-01,  1.7666e-01,  1.7611e-01,  1.7503e-01,\n",
       "         1.6808e-01,  1.6455e-01,  1.6000e-01,  1.5885e-01,  1.5833e-01,\n",
       "         1.4880e-01,  1.4085e-01,  1.4043e-01,  1.4034e-01,  1.3702e-01,\n",
       "         1.3610e-01,  1.3550e-01,  1.2189e-01,  1.1901e-01,  1.1852e-01,\n",
       "         1.1644e-01,  1.1311e-01,  1.1254e-01,  1.0876e-01,  1.0614e-01,\n",
       "         1.0590e-01,  1.0068e-01,  9.7648e-02,  9.6856e-02,  9.3758e-02,\n",
       "         9.3073e-02,  8.2077e-02,  7.7877e-02,  7.6893e-02,  7.5612e-02,\n",
       "         7.4929e-02,  7.3630e-02,  7.3235e-02,  7.2090e-02,  6.8975e-02,\n",
       "         6.6102e-02,  6.5069e-02,  6.4252e-02,  6.2784e-02,  6.1518e-02,\n",
       "         5.9066e-02,  5.5901e-02,  5.1771e-02,  5.0637e-02,  4.9579e-02,\n",
       "         4.6996e-02,  4.5175e-02,  4.2470e-02,  3.9474e-02,  3.8928e-02,\n",
       "         3.8773e-02,  3.5331e-02,  3.4927e-02,  3.4817e-02,  3.2096e-02,\n",
       "         3.1570e-02,  3.1035e-02,  3.0772e-02,  2.8909e-02,  2.8552e-02,\n",
       "         2.7711e-02,  2.5357e-02,  2.3727e-02,  2.2280e-02,  2.2190e-02,\n",
       "         2.0596e-02,  1.7748e-02,  1.1678e-02,  1.1610e-02,  1.1177e-02,\n",
       "         1.0655e-02,  8.9490e-03,  4.3751e-03, -9.0396e-04, -7.6871e-03,\n",
       "        -8.9366e-03, -1.0799e-02, -1.3832e-02, -1.7925e-02, -2.4924e-02,\n",
       "        -2.5194e-02, -2.5544e-02, -3.1587e-02, -3.1949e-02, -3.5950e-02,\n",
       "        -3.8982e-02, -4.2794e-02, -4.6053e-02, -4.6695e-02, -5.3590e-02,\n",
       "        -5.5052e-02, -6.3890e-02, -6.6655e-02, -6.8121e-02, -6.8503e-02,\n",
       "        -6.8541e-02, -7.1962e-02, -7.2669e-02, -7.2976e-02, -7.3879e-02,\n",
       "        -7.9246e-02, -8.4354e-02, -8.5221e-02, -8.5909e-02, -8.9369e-02,\n",
       "        -9.0427e-02, -9.4862e-02, -9.7279e-02, -9.7438e-02, -9.8312e-02,\n",
       "        -9.9780e-02, -1.0525e-01, -1.0809e-01, -1.0946e-01, -1.1585e-01,\n",
       "        -1.1800e-01, -1.2240e-01, -1.2261e-01, -1.2390e-01, -1.2680e-01,\n",
       "        -1.3146e-01, -1.3246e-01, -1.3583e-01, -1.4020e-01, -1.4469e-01,\n",
       "        -1.4544e-01, -1.4803e-01, -1.4965e-01, -1.5352e-01, -1.5646e-01,\n",
       "        -1.5874e-01, -1.6490e-01, -1.6490e-01, -1.7242e-01, -1.7520e-01,\n",
       "        -1.7562e-01, -1.7619e-01, -1.8214e-01, -1.8692e-01, -1.8798e-01,\n",
       "        -1.9240e-01, -1.9255e-01, -1.9257e-01, -1.9405e-01, -1.9500e-01,\n",
       "        -1.9522e-01, -1.9586e-01, -1.9994e-01, -2.0401e-01, -2.0972e-01,\n",
       "        -2.1056e-01, -2.1107e-01, -2.1221e-01, -2.1641e-01, -2.1922e-01,\n",
       "        -2.2103e-01, -2.2349e-01, -2.2679e-01, -2.2980e-01, -2.3168e-01,\n",
       "        -2.3840e-01, -2.4228e-01, -2.4280e-01, -2.4448e-01, -2.4911e-01,\n",
       "        -2.5082e-01, -2.5176e-01, -2.5178e-01, -2.5227e-01, -2.5442e-01,\n",
       "        -2.6457e-01, -2.6601e-01, -2.6654e-01, -2.6884e-01, -2.6995e-01,\n",
       "        -2.7213e-01, -2.7306e-01, -2.7803e-01, -2.7980e-01, -2.8261e-01,\n",
       "        -2.8502e-01, -2.8599e-01, -2.8781e-01, -2.9084e-01, -3.0091e-01,\n",
       "        -3.0155e-01, -3.0529e-01, -3.0530e-01, -3.0540e-01, -3.0645e-01,\n",
       "        -3.0966e-01, -3.1168e-01, -3.1214e-01, -3.1507e-01, -3.1892e-01,\n",
       "        -3.2154e-01, -3.2220e-01, -3.2382e-01, -3.2490e-01, -3.3589e-01,\n",
       "        -3.3629e-01, -3.3648e-01, -3.4099e-01, -3.4329e-01, -3.4334e-01,\n",
       "        -3.4336e-01, -3.4368e-01, -3.4441e-01, -3.4556e-01, -3.4731e-01,\n",
       "        -3.4871e-01, -3.4898e-01, -3.4963e-01, -3.5641e-01, -3.5658e-01,\n",
       "        -3.6481e-01, -3.6783e-01, -3.7052e-01, -3.7267e-01, -3.7339e-01,\n",
       "        -3.7366e-01, -3.7545e-01, -3.7958e-01, -3.8588e-01, -3.8846e-01,\n",
       "        -3.8892e-01, -3.8986e-01, -3.9432e-01, -3.9681e-01, -4.0019e-01,\n",
       "        -4.0584e-01, -4.0981e-01, -4.1568e-01, -4.1915e-01, -4.2158e-01,\n",
       "        -4.2975e-01, -4.3121e-01, -4.3330e-01, -4.3590e-01, -4.3646e-01,\n",
       "        -4.3667e-01, -4.3693e-01, -4.3735e-01, -4.4003e-01, -4.4577e-01,\n",
       "        -4.5014e-01, -4.5130e-01, -4.5181e-01, -4.5305e-01, -4.5553e-01,\n",
       "        -4.6460e-01, -4.6690e-01, -4.6985e-01, -4.8075e-01, -4.8085e-01,\n",
       "        -4.8602e-01, -4.8943e-01, -4.9600e-01, -4.9634e-01, -4.9709e-01,\n",
       "        -5.0009e-01, -5.0072e-01, -5.0072e-01, -5.0401e-01, -5.0560e-01,\n",
       "        -5.1171e-01, -5.1337e-01, -5.1738e-01, -5.1802e-01, -5.1916e-01,\n",
       "        -5.2012e-01, -5.2771e-01, -5.3018e-01, -5.3107e-01, -5.3479e-01,\n",
       "        -5.3521e-01, -5.3575e-01, -5.3698e-01, -5.4071e-01, -5.4386e-01,\n",
       "        -5.4568e-01, -5.4698e-01, -5.5885e-01, -5.6008e-01, -5.6483e-01,\n",
       "        -5.6624e-01, -5.8689e-01, -5.9063e-01, -5.9158e-01, -5.9319e-01,\n",
       "        -5.9339e-01, -5.9503e-01, -5.9526e-01, -5.9593e-01, -5.9760e-01,\n",
       "        -6.0525e-01, -6.0571e-01, -6.0575e-01, -6.1478e-01, -6.1966e-01,\n",
       "        -6.2296e-01, -6.2419e-01, -6.2639e-01, -6.2676e-01, -6.2963e-01,\n",
       "        -6.4555e-01, -6.4658e-01, -6.4839e-01, -6.4920e-01, -6.4933e-01,\n",
       "        -6.5323e-01, -6.5923e-01, -6.6059e-01, -6.6404e-01, -6.6681e-01,\n",
       "        -6.7022e-01, -6.7640e-01, -6.7764e-01, -6.7966e-01, -6.8465e-01,\n",
       "        -6.9013e-01, -6.9556e-01, -6.9862e-01, -7.0285e-01, -7.0888e-01,\n",
       "        -7.1481e-01, -7.2079e-01, -7.2270e-01, -7.2717e-01, -7.2951e-01,\n",
       "        -7.3536e-01, -7.4023e-01, -7.4311e-01, -7.4765e-01, -7.5246e-01,\n",
       "        -7.5313e-01, -7.5417e-01, -7.5490e-01, -7.5823e-01, -7.6327e-01,\n",
       "        -7.6425e-01, -7.6870e-01, -7.7489e-01, -7.7661e-01, -7.7831e-01,\n",
       "        -7.8231e-01, -7.8368e-01, -7.8851e-01, -7.8968e-01, -7.9031e-01,\n",
       "        -7.9145e-01, -7.9154e-01, -7.9340e-01, -7.9547e-01, -7.9914e-01,\n",
       "        -8.0086e-01, -8.0413e-01, -8.0430e-01, -8.0551e-01, -8.1192e-01,\n",
       "        -8.1890e-01, -8.2159e-01, -8.2779e-01, -8.2874e-01, -8.2926e-01,\n",
       "        -8.3046e-01, -8.3204e-01, -8.3385e-01, -8.4296e-01, -8.4485e-01,\n",
       "        -8.4540e-01, -8.4858e-01, -8.5674e-01, -8.5888e-01, -8.6051e-01,\n",
       "        -8.6132e-01, -8.6178e-01, -8.6330e-01, -8.6391e-01, -8.6792e-01,\n",
       "        -8.6891e-01, -8.7052e-01, -8.7892e-01, -8.8276e-01, -8.8337e-01,\n",
       "        -9.0087e-01, -9.0459e-01, -9.0526e-01, -9.0727e-01, -9.0963e-01,\n",
       "        -9.1866e-01, -9.2231e-01, -9.2553e-01, -9.3090e-01, -9.3382e-01,\n",
       "        -9.3952e-01, -9.4305e-01, -9.5478e-01, -9.5504e-01, -9.5737e-01,\n",
       "        -9.5945e-01, -9.6099e-01, -9.6483e-01, -9.6506e-01, -9.6817e-01,\n",
       "        -9.7241e-01, -9.8016e-01, -9.8330e-01, -9.8595e-01, -9.8843e-01,\n",
       "        -9.9197e-01, -9.9313e-01, -9.9527e-01, -1.0005e+00, -1.0077e+00,\n",
       "        -1.0077e+00, -1.0082e+00, -1.0090e+00, -1.0092e+00, -1.0112e+00,\n",
       "        -1.0112e+00, -1.0113e+00, -1.0177e+00, -1.0201e+00, -1.0302e+00,\n",
       "        -1.0494e+00, -1.0584e+00, -1.0670e+00, -1.0690e+00, -1.0704e+00,\n",
       "        -1.0733e+00, -1.0734e+00, -1.0754e+00, -1.0765e+00, -1.0766e+00,\n",
       "        -1.0959e+00, -1.0981e+00, -1.1045e+00, -1.1080e+00, -1.1112e+00,\n",
       "        -1.1131e+00, -1.1166e+00, -1.1193e+00, -1.1236e+00, -1.1267e+00,\n",
       "        -1.1300e+00, -1.1387e+00, -1.1396e+00, -1.1488e+00, -1.1512e+00,\n",
       "        -1.1605e+00, -1.1641e+00, -1.1716e+00, -1.1748e+00, -1.1811e+00,\n",
       "        -1.1826e+00, -1.1872e+00, -1.1962e+00, -1.2039e+00, -1.2049e+00,\n",
       "        -1.2062e+00, -1.2143e+00, -1.2205e+00, -1.2207e+00, -1.2220e+00,\n",
       "        -1.2309e+00, -1.2375e+00, -1.2410e+00, -1.2416e+00, -1.2446e+00,\n",
       "        -1.2464e+00, -1.2600e+00, -1.2653e+00, -1.2673e+00, -1.2694e+00,\n",
       "        -1.2705e+00, -1.2764e+00, -1.2777e+00, -1.2808e+00, -1.2812e+00,\n",
       "        -1.2935e+00, -1.2969e+00, -1.3189e+00, -1.3201e+00, -1.3312e+00,\n",
       "        -1.3383e+00, -1.3416e+00, -1.3474e+00, -1.3499e+00, -1.3508e+00,\n",
       "        -1.3676e+00, -1.3816e+00, -1.3864e+00, -1.4040e+00, -1.4071e+00,\n",
       "        -1.4115e+00, -1.4169e+00, -1.4216e+00, -1.4253e+00, -1.4270e+00,\n",
       "        -1.4510e+00, -1.4539e+00, -1.4540e+00, -1.4544e+00, -1.4610e+00,\n",
       "        -1.4649e+00, -1.4666e+00, -1.4669e+00, -1.4699e+00, -1.4844e+00,\n",
       "        -1.4868e+00, -1.4878e+00, -1.4902e+00, -1.4966e+00, -1.5062e+00,\n",
       "        -1.5151e+00, -1.5165e+00, -1.5225e+00, -1.5396e+00, -1.5461e+00,\n",
       "        -1.5478e+00, -1.5483e+00, -1.5534e+00, -1.5590e+00, -1.5705e+00,\n",
       "        -1.5728e+00, -1.5850e+00, -1.6002e+00, -1.6075e+00, -1.6160e+00,\n",
       "        -1.6215e+00, -1.6252e+00, -1.6297e+00, -1.6343e+00, -1.6444e+00,\n",
       "        -1.6456e+00, -1.6515e+00, -1.6555e+00, -1.6657e+00, -1.6699e+00,\n",
       "        -1.6854e+00, -1.6958e+00, -1.7036e+00, -1.7430e+00, -1.7484e+00,\n",
       "        -1.7536e+00, -1.7677e+00, -1.7807e+00, -1.7986e+00, -1.8052e+00,\n",
       "        -1.8455e+00, -1.8531e+00, -1.9134e+00, -1.9136e+00, -1.9608e+00,\n",
       "        -1.9854e+00, -1.9916e+00, -2.0239e+00, -2.0635e+00, -2.0647e+00,\n",
       "        -2.0688e+00, -2.0741e+00, -2.0803e+00, -2.1015e+00, -2.1237e+00,\n",
       "        -2.1837e+00, -2.1939e+00, -2.2061e+00, -2.2348e+00, -2.2535e+00,\n",
       "        -2.4500e+00, -2.4925e+00, -2.6718e+00, -2.7807e+00, -2.8237e+00])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dde5bac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading csv files...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done~!\n",
      "loading model...........\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00622bf473ac449c8ce3012dd4358c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from fastchat.model.model_adapter import (\n",
    "    load_model,\n",
    "    get_conversation_template,\n",
    ")\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import argparse\n",
    "import numpy as np\n",
    "#from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM\n",
    "#from ftmodule.model import HFlikemodel\n",
    "from transformers import StoppingCriteria,StoppingCriteriaList\n",
    "from script.query_llm_model_inference import generate_with_start_kvcache\n",
    "\n",
    "\n",
    "\n",
    "print(\"loading csv files...........\")\n",
    "sentense_ids = pd.read_csv(\"data/unarXive_quantum_physics/unarXive_quantum_physics.clear.sections.id.csv\")\n",
    "print(\"done~!\")\n",
    "need_question_ids = list(range(len(sentense_ids)))\n",
    "\n",
    "# print(\"loading finished ids.........\")\n",
    "# with open(\"data/unarXive.clear/query.question.results.good_questions.ids.json\", 'r') as f:good_question_ids = json.load(f)\n",
    "# print(\"done~!\")\n",
    "# good_question_ids=set(good_question_ids)\n",
    "# need_question_ids=set(list(range(len(sentense_ids)))) - good_question_ids\n",
    "# need_question_ids = list(need_question_ids)\n",
    "# print(f\"remain {len(need_question_ids)}/{len(sentense_ids)} items\")\n",
    "\n",
    "SAVEPATH = \"data/unarXive_quantum_physics/question_results\"\n",
    "sectionsf = h5py.File('data/unarXive_quantum_physics/unarXive_quantum_physics.clear.sections.h5', 'r')\n",
    "# abstractf = h5py.File('data/unarXive.clear/unarXive.clear.abstract.h5', 'r')\n",
    "titlef    = h5py.File('data/unarXive_quantum_physics/unarXive_quantum_physics.clear.title.h5', 'r')\n",
    "\n",
    "print(\"loading model...........\")\n",
    "#model_path = 'pretrain_weights/fast_t5'\n",
    "model_path = 'pretrain_weights/vicuna-7b-v1.1'\n",
    "model, tokenizer = load_model(\n",
    "    model_path,\n",
    "    \"cuda\",\n",
    "    1,\n",
    "    load_8bit=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "066231f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "__id= 0\n",
    "tokenizer.padding_side='left'\n",
    "_id        = need_question_ids[__id]\n",
    "data       = sentense_ids.iloc[_id]\n",
    "paper_id   = data['paper_id']\n",
    "sentence_id= data['section_num']\n",
    "\n",
    "\n",
    "#abstract = abstractf.get(f'abstract/{paper_id}')[()].decode('utf-8')\n",
    "title    = titlef.get(f'abstract/{paper_id}')[()].decode('utf-8')\n",
    "sentense = sectionsf.get(f'{paper_id}/{sentence_id}')[()].decode('utf-8')\n",
    "\n",
    "#sentense = \" \".join(sentense.split(\" \")[:max_length])\n",
    "conv = get_conversation_template(model_path)\n",
    "#qs = f\"\"\"Read below sentence and tell me its type. The answer should be one word and is one of type from ['Author List', 'Reference', 'Content', 'Meaningless']. There is the sentence \\\"{sentense}\\\" \"\"\"\n",
    "qs = f\"\"\"I have a specific paragraph from a scholarly paper named <{title}>. I need your help to formulate an insightful question based on the information given. The specific paragraph from the paper is:\\n\\\"\\\"\\\"\\n{sentense} \\n\\\"\\\"\\\"\\nFollowing the question, provide a one-sentence response that succinctly answers it. The response should be start with \"What is\". Make the response as short as possible. \"\"\"\n",
    "conv.append_message(conv.roles[0], qs)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompts = [conv.get_prompt()]\n",
    "#print(prompts[0])\n",
    "input_ids = tokenizer(prompts).input_ids\n",
    "input_ids = torch.as_tensor(input_ids).cuda()\n",
    "#prompts    = [get_prompt(_id)] #(B=1)\n",
    "#input_ids_f= tokenizer(prompts).input_ids\n",
    "# tokenizer.pad_token = tokenizer.unk_token\n",
    "# inputs_ids = tokenizer.pad({'input_ids': input_ids_f},padding='longest',\n",
    "#                             max_length=512,pad_to_multiple_of=8,return_attention_mask=False,\n",
    "#                             return_tensors='pt').input_ids.cuda()\n",
    "length = len(input_ids[0])\n",
    "# with torch.no_grad():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5712718",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#conv.append_message(conv.roles[1], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8716fa95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER:\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f0c2c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastchat.serve.inference import *\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_with_start_kvcache(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    params: Dict,\n",
    "    device: str,\n",
    "    context_len: int,\n",
    "    stream_interval: int = 2,\n",
    "    judge_sent_end: bool = False,\n",
    "    start_kvcache= None,\n",
    "    return_kvcache:bool=False\n",
    "):\n",
    "    if hasattr(model, \"device\"):device = model.device\n",
    "\n",
    "    # Read parameters\n",
    "    prompt = params[\"prompt\"]\n",
    "    len_prompt = len(prompt)\n",
    "    temperature = float(params.get(\"temperature\", 1.0))\n",
    "    repetition_penalty = float(params.get(\"repetition_penalty\", 1.0))\n",
    "    top_p = float(params.get(\"top_p\", 1.0))\n",
    "    top_k = int(params.get(\"top_k\", -1))  # -1 means disable\n",
    "    max_new_tokens = int(params.get(\"max_new_tokens\", 256))\n",
    "    echo = bool(params.get(\"echo\", False))\n",
    "    stop_str = params.get(\"stop\", None)\n",
    "    stop_token_ids = params.get(\"stop_token_ids\", None) or []\n",
    "    stop_token_ids.append(tokenizer.eos_token_id)\n",
    "\n",
    "    logits_processor = prepare_logits_processor(\n",
    "        temperature, repetition_penalty, top_p, top_k\n",
    "    )\n",
    "    input_ids = tokenizer(prompt).input_ids\n",
    "\n",
    "    if model.config.is_encoder_decoder:\n",
    "        max_src_len = context_len\n",
    "    else:  # truncate\n",
    "        max_src_len = context_len - max_new_tokens - 1\n",
    "\n",
    "    input_ids = input_ids[-max_src_len:]\n",
    "\n",
    "    output_ids = list(input_ids)\n",
    "    input_echo_len = len(input_ids)\n",
    "\n",
    "    if model.config.is_encoder_decoder:\n",
    "        encoder_output = model.encoder(input_ids=torch.as_tensor([input_ids], device=device))[0]\n",
    "        start_ids = torch.as_tensor(\n",
    "            [[model.generation_config.decoder_start_token_id]],\n",
    "            dtype=torch.int64,\n",
    "            device=device,\n",
    "        )\n",
    "    out = None\n",
    "    \n",
    "    past_key_values = start_kvcache\n",
    "    sent_interrupt = False\n",
    "    for i in range(max_new_tokens):\n",
    "        if i == 0:  # prefill\n",
    "            if model.config.is_encoder_decoder:\n",
    "                out = model.decoder(input_ids=start_ids,encoder_hidden_states=encoder_output,use_cache=True)\n",
    "                logits = model.lm_head(out[0])\n",
    "            else:\n",
    "                \n",
    "                cover_token = past_key_values[-1][0].shape[-2] if past_key_values is not None else 0\n",
    "                out = model(torch.as_tensor([input_ids[cover_token:]], device=device), use_cache=True,past_key_values=past_key_values)\n",
    "                logits = out.logits\n",
    "            past_key_values = out.past_key_values\n",
    "        else:  # decoding\n",
    "            if model.config.is_encoder_decoder:\n",
    "                out = model.decoder(\n",
    "                    input_ids=torch.as_tensor([[token] if not sent_interrupt else output_ids],device=device,),\n",
    "                    encoder_hidden_states=encoder_output,\n",
    "                    use_cache=True,\n",
    "                    past_key_values=past_key_values if not sent_interrupt else None,\n",
    "                )\n",
    "                sent_interrupt = False\n",
    "                logits = model.lm_head(out[0])\n",
    "            else:\n",
    "                out = model(\n",
    "                    input_ids=torch.as_tensor([[token] if not sent_interrupt else output_ids],device=device,),\n",
    "                    use_cache=True,\n",
    "                    past_key_values=past_key_values if not sent_interrupt else None,\n",
    "                )\n",
    "                sent_interrupt = False\n",
    "                logits = out.logits\n",
    "            past_key_values = out.past_key_values\n",
    "\n",
    "        if logits_processor:\n",
    "            if repetition_penalty > 1.0:\n",
    "                tmp_output_ids = torch.as_tensor([output_ids], device=logits.device)\n",
    "            else:\n",
    "                tmp_output_ids = None\n",
    "            last_token_logits = logits_processor(tmp_output_ids, logits[:, -1, :])[0]\n",
    "        else:\n",
    "            last_token_logits = logits[0, -1, :]\n",
    "\n",
    "        if device == \"mps\":\n",
    "            # Switch to CPU by avoiding some bugs in mps backend.\n",
    "            last_token_logits = last_token_logits.float().to(\"cpu\")\n",
    "\n",
    "        if temperature < 1e-5 or top_p < 1e-8:  # greedy\n",
    "            _, indices = torch.topk(last_token_logits, 2)\n",
    "            tokens = [int(index) for index in indices.tolist()]\n",
    "        else:\n",
    "            probs = torch.softmax(last_token_logits, dim=-1)\n",
    "            indices = torch.multinomial(probs, num_samples=2)\n",
    "            tokens = [int(token) for token in indices.tolist()]\n",
    "        token = tokens[0]\n",
    "        output_ids.append(token)\n",
    "\n",
    "        if token in stop_token_ids:\n",
    "            stopped = True\n",
    "        else:\n",
    "            stopped = False\n",
    "        \n",
    "        # Yield the output tokens\n",
    "        if stopped:\n",
    "            break\n",
    "    #print(len(output_ids))\n",
    "    tmp_output_ids = output_ids[input_echo_len:]\n",
    "    output = tokenizer.decode(\n",
    "                tmp_output_ids,\n",
    "                skip_special_tokens=True,\n",
    "                spaces_between_special_tokens=False,\n",
    "                clean_up_tokenization_spaces=True,\n",
    "            )\n",
    "    # Finish stream event, which contains finish reason\n",
    "    if i == max_new_tokens - 1:\n",
    "        finish_reason = \"length\"\n",
    "    elif stopped:\n",
    "        finish_reason = \"stop\"\n",
    "    else:\n",
    "        finish_reason = None\n",
    "    \n",
    "    output = {\n",
    "        \"text\": output,\n",
    "        \"usage\": {\n",
    "            \"prompt_tokens\": input_echo_len,\n",
    "            \"completion_tokens\": i,\n",
    "            \"total_tokens\": input_echo_len + i,\n",
    "        },\n",
    "        \"finish_reason\": finish_reason,\n",
    "    }\n",
    "\n",
    "    if return_kvcache:\n",
    "        output['last_kvcache'] = past_key_values\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "85d38bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_kvcache[-1][0].shape[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6d50da06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER:\n"
     ]
    }
   ],
   "source": [
    "conv = get_conversation_template(model_path)\n",
    "conv.append_message(conv.roles[0], \"\")\n",
    "print(conv.get_prompt() )\n",
    "prefix_kvcache = generate_with_start_kvcache(model,tokenizer,{'prompt':conv.get_prompt() ,'max_length':128},'cuda',context_len=16000,return_kvcache=True)['last_kvcache']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "486dcfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(prefix_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "894af6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = \"Who are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "008c6e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = f\"\"\"I have a specific paragraph from a scholarly paper named <{title}>. I need your help to formulate an insightful question based on the information given. The specific paragraph from the paper is:\\n\\\"\\\"\\\"\\n{sentense} \\n\\\"\\\"\\\"\\nFollowing the question, provide a one-sentence response that succinctly answers it. The response should be start with \"What is\". Make the response as short as possible. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4565d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conv = get_conversation_template(model_path)\n",
    "conv.append_message(conv.roles[0], qs)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "result = generate_with_start_kvcache(model,tokenizer,{'prompt':conv.get_prompt(),'max_new_tokens':32,'temperature':0,'top_k':1},'cuda',context_len=16000,return_kvcache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2ff3a215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 232, 128])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['last_kvcache'][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5120d096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490 ms ± 1.71 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "conv = get_conversation_template(model_path)\n",
    "conv.append_message(conv.roles[0], qs)\n",
    "conv.append_message(conv.roles[1], result[\"text\"])\n",
    "conv.append_message(conv.roles[0], \"Provide another question\")\n",
    "conv.append_message(conv.roles[1], None)\n",
    "generate_with_start_kvcache(model,tokenizer,{'prompt':conv.get_prompt(),'max_new_tokens':32,'temperature':0,'top_k':1},'cuda',context_len=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "201a2054",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "458 ms ± 8.41 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "conv = get_conversation_template(model_path)\n",
    "conv.append_message(conv.roles[0], qs)\n",
    "conv.append_message(conv.roles[1], result[\"text\"])\n",
    "conv.append_message(conv.roles[0], \"Provide another question\")\n",
    "conv.append_message(conv.roles[1], None)\n",
    "generate_with_start_kvcache(model,tokenizer,{'prompt':conv.get_prompt(),'max_length':32,'temperature':0,'top_k':1},'cuda',context_len=16000,start_kvcache=result['last_kvcache'])['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e956ecca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'What is the significance of the focus on \"standards fields\" and lack of reference to strings in the quantum theory of particles and fields?',\n",
       " 'usage': {'prompt_tokens': 544, 'completion_tokens': 28, 'total_tokens': 572},\n",
       " 'finish_reason': 'stop'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
