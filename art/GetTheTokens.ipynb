{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "054e20ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f81e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5abdf6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span>device = <span style=\"color: #808000; text-decoration-color: #808000\">f\"cuda:0\"</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 #from bits4llama.llama import load_quant</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3 model = load_quant(<span style=\"color: #808000; text-decoration-color: #808000\">\"pretrain_weights/vicuna/vicuna-7b-v1.1\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"pretrain_weights/vicuna/vi</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 # model = LlamaForCausalLM.from_pretrained(</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 #     \"internlm/internlm-7b\", low_cpu_mem_usage=True, torch_dtype=torch.float16).cuda()</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'load_quant'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m3\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0mdevice = \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mcuda:0\u001b[0m\u001b[33m\"\u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m\u001b[2m#from bits4llama.llama import load_quant\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3 model = load_quant(\u001b[33m\"\u001b[0m\u001b[33mpretrain_weights/vicuna/vicuna-7b-v1.1\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mpretrain_weights/vicuna/vi\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m\u001b[2m# model = LlamaForCausalLM.from_pretrained(\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0m\u001b[2m#     \"internlm/internlm-7b\", low_cpu_mem_usage=True, torch_dtype=torch.float16).cuda()\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m6 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'load_quant'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = f\"cuda:0\"\n",
    "#from bits4llama.llama import load_quant\n",
    "#model = load_quant(\"pretrain_weights/vicuna/vicuna-7b-v1.1\", \"pretrain_weights/vicuna/vicuna-7b-v1.1/vicuna7b-4bit-128g.pt\", 4, 128,device=device).to(device)\n",
    "# model = LlamaForCausalLM.from_pretrained(\n",
    "#     \"internlm/internlm-7b\", low_cpu_mem_usage=True, torch_dtype=torch.float16).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f01590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrain_weights/vicuna-7b-v1.1\n",
      "pretrain_weights/vicuna-7b-v1.1/pytorch_model.bin\n",
      "False\n",
      "pretrain_weights/vicuna-7b-v1.1/pytorch_model.bin.index.json\n",
      "True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1aa4c278f24648ac28c8a058f61f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastchat.model.model_adapter import (\n",
    "    load_model,\n",
    "    get_conversation_template,\n",
    ")\n",
    "model_path = 'pretrain_weights/vicuna-7b-v1.1'\n",
    "model, tokenizer = load_model(\n",
    "    model_path,\n",
    "    \"cuda\",\n",
    "    1,\n",
    "    load_8bit=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebb81dbb",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Given the passage: \"\"\"This article presents a model of general-purpose computing on a semantic network substrate. The concepts presented are applicable to any semantic network representation. However, due to the standards and technological infrastructure devoted to the Semantic Web effort, this article is presented from this point of view. In the proposed model of computing, the application programming interface, the run-time program, and the state of the computing virtual machine are all represented in the Resource Description Framework (RDF). The implementation of the concepts presented provides a practical computing paradigm that leverages the highly-distributed and standardized representational-layer of the Semantic Web.\"\"\" Please check whether the passage is relative to the question: what is Semantic Network . Return Yes or No. ASSISTANT:\n",
      "tensor([ 3869, 29889,     2], device='cuda:0')\n",
      " Yes.</s>\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "conv = get_conv_template('vicuna_v1.1').copy()\n",
    "conv.append_message(conv.roles[0], \"\"\"Given the passage: \\\"\\\"\\\"This article presents a model of general-purpose computing on a semantic network substrate. The concepts presented are applicable to any semantic network representation. However, due to the standards and technological infrastructure devoted to the Semantic Web effort, this article is presented from this point of view. In the proposed model of computing, the application programming interface, the run-time program, and the state of the computing virtual machine are all represented in the Resource Description Framework (RDF). The implementation of the concepts presented provides a practical computing paradigm that leverages the highly-distributed and standardized representational-layer of the Semantic Web.\\\"\\\"\\\" Please check whether the passage is relative to the question: what is Semantic Network . Return Yes or No.\"\"\")\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "input_ids = tokenizer([prompt]).input_ids\n",
    "input_ids = torch.as_tensor(input_ids).cuda()\n",
    "#output_ids = model.generate(input_ids,max_length=600)\n",
    "# print(prompt)\n",
    "# print(output_ids[0][len(input_ids[0]):])\n",
    "# print(tokenizer.decode(output_ids[0][len(input_ids[0]):]))\n",
    "# print(\"==================\")\n",
    "output_ids = model.generate(input_ids,return_dict_in_generate=True,output_scores=True,max_length=600)\n",
    "print(prompt)\n",
    "print(output_ids.sequences[0][len(input_ids[0]):])\n",
    "print(tokenizer.decode(output_ids.sequences[0][len(input_ids[0]):]))\n",
    "print(\"==================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6073189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3869"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3869"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfde2a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3869, device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(output_ids.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c2e04dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4036, 0.5962, 0.0000]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids.scores[0][:,[1939,3869,29889]].softmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c3f0a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dc134d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f5cc96bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "for (_id,paper_id,abstract,key_word_1,key_word_2,key_word_3,key_word_4,\n",
    "    key_word_5,key_word_6,key_word_7,key_word_8,key_word_9,key_word_10) in data:\n",
    "    new_data.append([paper_id,abstract.replace('\\n',' '),key_word_1,key_word_2,key_word_3,key_word_4,\n",
    "    key_word_5,key_word_6,key_word_7,key_word_8,key_word_9,key_word_10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6fe3a01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_titles = ['paper_id', 'abstract', 'key_word_1', 'key_word_2', 'key_word_3',\n",
    "                 'key_word_4', 'key_word_5', 'key_word_6', 'key_word_7', 'key_word_8',\n",
    "                 'key_word_9', 'key_word_10']\n",
    "df = pd.DataFrame(new_data, columns=column_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f1ace8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/unArXiv.key_word_from_abstract.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0bfd73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3c13e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_logits      = output_ids.scores[0][:,[1939,3869]] # the first output and the logists for No/Yes\n",
    "log_softmax    = F.log_softmax(lm_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cde5e56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1025]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax.norm(dim=1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9bfea23",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def vicuna_check_is_relative_template(title_ids, context_ids,MAX_SEQUENCE_LEN=512):\n",
    "        title_ids = title_ids[1:]    # remove the <s>\n",
    "        context_ids = context_ids[1:]  # remove the <s>\n",
    "        '''template is \n",
    "        <s>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Here is a passage about [<title_ids>]:\\n\"\"\"\\n<context_ids>\\n\"\"\"\\n Based on the given passage, formulate a question that aligns most accurately with the primary fact disclosed within the text. ASSISTANT:\n",
    "        '''\n",
    "        base = [1, 319, 13563, 1546, 263, 12758, 1404, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173,\n",
    "                        29892, 322, 1248, 568, 6089, 304, 278, 1404, 29915, 29879, 5155, 29889, 3148, 1001, 29901]\n",
    "        a = base + [11221, 278, 13382, 6160, 15945] \n",
    "        b = [15945, 29908, 3529, 1423, 3692, 278, 13382, 338, 6198, 304, 278, 1139, 29901, 825, 338]\n",
    "        c = [869, 7106, 3869, 470, 1939,29889,  319, 1799, 9047, 13566, 29901]\n",
    "        \n",
    "        complete_sequence  = a + context_ids + b + title_ids + c\n",
    "        exceed_length = len(complete_sequence) - MAX_SEQUENCE_LEN\n",
    "        if exceed_length >0: context_ids = context_ids[:-exceed_length]\n",
    "        return a + context_ids + b + title_ids + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c93ae29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  319,\n",
       "  13563,\n",
       "  1546,\n",
       "  263,\n",
       "  12758,\n",
       "  1404,\n",
       "  322,\n",
       "  385,\n",
       "  23116,\n",
       "  21082,\n",
       "  20255,\n",
       "  29889,\n",
       "  450,\n",
       "  20255,\n",
       "  4076,\n",
       "  8444,\n",
       "  29892,\n",
       "  13173,\n",
       "  29892,\n",
       "  322,\n",
       "  1248,\n",
       "  568,\n",
       "  6089,\n",
       "  304,\n",
       "  278,\n",
       "  1404,\n",
       "  29915,\n",
       "  29879,\n",
       "  5155,\n",
       "  29889,\n",
       "  3148,\n",
       "  1001,\n",
       "  29901,\n",
       "  2266,\n",
       "  338,\n",
       "  263,\n",
       "  13382,\n",
       "  515,\n",
       "  263,\n",
       "  16021,\n",
       "  5650,\n",
       "  29901]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Here is a passage from a scientific paper:\"]).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2808850e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We demonstrate that assuming the \"discrete\" vacuum geometry in the Minkowskian Higgs model with vacuum BPS monopole solutions can justify the Dirac fundamental quantization of that model. The important constituent of this quantization is getting various rotary effects, including collective solid rotations inside the physical BPS monopole vacuum, and just assuming the \"discrete\" vacuum geometry seems to be that thing able to justify these rotary effects. More precisely, assuming the \"discrete\" geometry for the appropriate vacuum manifold implies the presence of thread topological defects (side by side with point hedgehog topological defects and walls between different topological domains) inside this manifold in the shape of specific (rectilinear) threads: gauge and Higgs fields located in the spatial region intimately near the axis $z$ of the chosen (rest) reference frame. This serves as the source of collective solid rotations proceeding inside the BPS monopole vacuum suffered the Dirac fundamental quantization. It will be argued that indeed the first-order phase transition occurs in the Minkowskian Higgs model with vacuum BPS monopoles quantized by Dirac. This comes to the coexistence of two thermodynamic phases inside the appropriate BPS monopole vacuum. There are the thermodynamic phases of collective solid rotations and superfluid potential motions.\n"
     ]
    }
   ],
   "source": [
    "print('We demonstrate that assuming the \"discrete\" vacuum geometry in the\\nMinkowskian Higgs model with vacuum BPS monopole solutions can justify the\\nDirac fundamental quantization of that model. The important constituent of this\\nquantization is getting various rotary effects, including collective solid\\nrotations inside the physical BPS monopole vacuum, and just assuming the\\n\"discrete\" vacuum geometry seems to be that thing able to justify these rotary\\neffects. More precisely, assuming the \"discrete\" geometry for the appropriate\\nvacuum manifold implies the presence of thread topological defects (side by\\nside with point hedgehog topological defects and walls between different\\ntopological domains) inside this manifold in the shape of specific\\n(rectilinear) threads: gauge and Higgs fields located in the spatial region\\nintimately near the axis $z$ of the chosen (rest) reference frame. This serves\\nas the source of collective solid rotations proceeding inside the BPS monopole\\nvacuum suffered the Dirac fundamental quantization. It will be argued that\\nindeed the first-order phase transition occurs in the Minkowskian Higgs model\\nwith vacuum BPS monopoles quantized by Dirac. This comes to the coexistence of\\ntwo thermodynamic phases inside the appropriate BPS monopole vacuum. There are\\nthe thermodynamic phases of collective solid rotations and superfluid potential\\nmotions.'.replace('\\n',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43e29930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user\\'s questions. USER: Here is a passage from a scientific paper:\"\"\" mother is one of partents\"\"\" Based on the given passage, formulate a question that aligns most accurately with the primary fact disclosed within the text. ASSISTANT:'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer.decode(vicuna_only_passage_template(tokenizer.encode(\"what is mother\"), tokenizer.encode(\"mother is one of partents\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-tianning",
   "language": "python",
   "name": "pytorch-tianning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
