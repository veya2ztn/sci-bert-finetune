torchrun --nproc_per_node 1 --nnodes 1 --node_rank 0 --master_addr localhost --master_port 60000 create_doc_index.py \
--batch-size 32 --checkpoint-activations --seq-length 512 --seq-length-retriever 256 --max-position-embeddings 512 \
--load "" \
--indexed-evidence-bert-tokenized-data-path data/beir/data/evidence-beir-mmap/scifact-indexed-mmap/scifact-beir-evidence_text_document \
--indexed-title-bert-tokenized-data-path data/beir/data/evidence-beir-mmap/scifact-indexed-mmap/scifact-beir-evidence_title_document \
--embedding-path ./embedding-path/art-finetuning-embedding-beir/scifact-beir-checkpoints/m3e-base.pkl \
--indexer-log-interval 1000 --indexer-batch-size 128 --vocab-file ./bert-vocab/bert-large-uncased-vocab.txt \
--num-workers 2 --fp16 --max-training-rank 8 --num-layers 12 --hidden-size 768 \
--num-attention-heads 12 --kv-channels 64 --ffn-hidden-size 3072 \
--model-parallel-size 1 \
--retriever_model_name "moka-ai/m3e-base"